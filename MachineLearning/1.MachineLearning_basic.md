## Machine Learning 개념 다지기
> Machine Learning에 대한 개념과 학습 방법 등을 나타내는 페이지입니다.

> Machine Learning의 기초 개념을 배우고 Machine Learning Library인 Tensorflow에 대한 기초적인 학습을 진행합니다.

__Machine Learning__(머신 러닝)은 인공지능(AI) 중 한 분야로, 기계가 _자체적으로 학습_ 할 수 있도록 하는 알고리즘을 말한다. 머신 러닝을 통해서 기계는 코드로 작성하지 않아도 학습 데이터 셋으로부터 훈련을 하고, 이를 통해서 새로운 데이터를 처리할 수 있게 된다. Machine Learning의 학습 방법에는 __Supervised Learning__(지도 학습)과 __Unsupervised Learning__(비지도 학습)으로 나뉜다.

### _지도학습_ & _비지도학습_
__지도 학습__ 은 데이터에 대해서 _Label(정답)이 주어진 상태_ 에서 기계를 학습시키는 방법이다.(데이터, 레이블 형태) 예를 들어, 필기체 인식을 하는 알고리즘을 구성한다고 하면, '이 글자는 어떤 글자(숫자)다.' 라고 미리 정답(기준)을 알려준 상태에서 학습을 하는 것이라고 보면 된다. 정답을 기준 삼아서 기계는 학습을 하게 되고, 끝나고 난 후, 임의의 데이터를 통해서 얼마나 정확히 예측하는지 비교를 할 수 있다. 지도 학습에는 학습을 통해 _연속적인 값을 예측_ 하는 __Regression__(회귀 분석), _어떤 종류의 값인지를 표시_ 하는 __Classification__(분류) 등이 있다.

반대로, __비지도 학습__ 은 데이터에 대해서 _Label(정답)이 주어지지 않은 상태_ 로 기계를 학습시키는 방법이다.(데이터 형태) 예를 들어, 필기체 인식을 하는 알고리즘을 구성할 때, 데이터만 주고 이것들을 비슷한 형태로 분류를 하고 나서 집단화를 하는 것이다. 올바르게 학습을 했을 때, 기계는 필기체 데이터를 각 각의 숫자 및 글자에 맞는 그룹을 형성하게 된다. 비지도 학습에는 _비슷한 군집을 추정_ 하는 __Clustering__(군집화)과 _데이터를 새롭게 표현하여 원래 데이터보다 쉽게 해석_ 할 수 있도록 하는 __Unsupervised Transformation__(비지도 변환)등이 있다.

### _Tensorflow_ - Machine Learning Library

![Alt Text][Tensorflow_logo]

앞으로 Machine Learning 학습에 대한 진행을 Tensorflow를 통해 진행할 예정이다. Tensorflow는 Google의 기계 학습 연구 팀 소속의 Google Brain 팀이 개발한 Machine Learning과 Deep Neural Network를 위한 Open-Source Software Library이다.

[Tensorflow 설치 사이트 바로가기](https://www.tensorflow.org/install/)

Tensorflow는 __Data Flow Graph__(데이터 플로우 그래프) 방식을 사용하였다. 데이터 플로우 그래프란 수학 계산과 데이터의 흐름을 __Node__(노드)와 __Edge__(엣지)를 사용한 __Directed Graph__ 로 표현한 것이다. 여기서 Node는 _수학적 계산, 데이터의 입출력, 데이터의 읽기 및 저장_ 기능을 수행한다. 그리고 Edge는 _Node 간의 데이터 입출력 관계_ 를 나타낸다. Edge는 동적 사이즈의 _다차원 데이터 배열(=Tensor)_ 을 실어 나르는데, Tensorflow의 어원은 여기서 나왔다.

- Tensorflow의 특징
    + __Data Flow Graph__ 를 통한 풍부한 표현력
    + 코드의 수정 없이 __CPU / GPU__ 모드 동작
    + _Idea Test_ 단계에서 _Service_ 단계까지 이용 가능
    + __계산 구조__ 와 __목표 함수__ 만 정의해서 _자동으로 미분 계산 처리_ 가능
    + __Python / C++__ 지원, __SWIG__ 을 통한 다양한 언어 지원 가능
    
일반적으로 텐서플로우의 알고리즘의 단계는 다음과 같다.

1. __Placeholder__ 를 통한 데이터 수집, 발생, 혹은 데이터 파이프라인 설정
2. 계산 그래프를 통한 __데이터 입력__
3. __손실 함수__ (loss function)로 출력 데이터 평가
4. 변수 조정을 위한 __역전파__ 사용
5. 조건이 멈출 때까지 반복

![Alt Text][tf_algo]

### Tensorflow 기초 개념

#### Tensor
__Tensor__ (텐서) 란 계산 그래프를 구동하기 위해 Tensorflow가 사용하는 자료 구조로 정영화된 다차원 배열이다. Tensor는 변수로 선언할 수 있고, Placeholder로도 선언이 가능하다. Tensor를 생성하는 방법은 다음과 같다.
1. __고정 텐서__ : 특정한 값으로 정해진 텐서
2. __형태가 비슷한 텐서__ : 기존 텐서의 형태를 바탕으로 텐서 변수 초기화
3. __순열 텐서__ : 구간을 지정하는 방식으로 텐서를 선언
4. __랜덤 텐서__ : 랜덤 값으로 초기화된 텐서

```python
### 각종 텐서 선언 예제
## 1. 고정텐서
zero_tsr = tf.zeros([row_dim, col_dim]) # 0값으로 채워진 텐서
ones_tsr = tf.ones([row_dim, col_dim]) # 1값으로 채워진 텐서
filled_tsr = tf.fill([row_dim, col_dim], 42) # 동일한 상수(42) 값으로 채워진 텐서

## 2. 형태가 비슷한 텐서
constant_tsr = tf.constant([1,2,3])
zeros_similar = tf.zeros_like(constant_tsr)

## 3. 순열 텐서
### 아래 예제에서 start와 stop을 0과 1로 선언을 하게 되면 데이터 형태에서 오류가 난다.
### Value passed to parameter 'start' has DataType int32 not in list of allowed values : float32, float64
linear_tsr = tf.linspace(start = 0.0, stop = 1.0, start = 3) # 결과 값 : [0.0, 0.5, 1.0]
integer_seq_tsr = tf.range(start = 6, limit = 15, delta = 3) # 결과 값 : [6, 9, 12]
### linspace() 함수에는 마지막 경계 값이 포함되어 있고, range() 함수에는 마지막 경계값(limit)이 포함되어 있지 않다.

## 4. 랜덤 텐서
randunif_tsr = tf.random_uniform([row_dim, col_dim], minval=0, maxval=1) # minval <= 임의 값 < maxval 균등 분포를 따르는 난수
random_tsr = tf.random_normal([row_dim, col_dim], mean=0.0, stddev=1.0) # 정규 분포를 따르는 임의 숫자들로 텐서 생성
```

#### Variable
__Variable__ (변수) 란 텐서를 입력받아 변수를 출력하는 Variable()함수를 이용하여 생성한다. Variable() 함수는 변수 선언만 하기 때문에 초기화를 별도로 진행을 해주어야 한다.

```python
### 변수 선언 예제
my_var = tf.Variable(tf.zeros([2,3])) # Variable 메소드 : 변수 선언만 진행
sess = tf.Session() # 값을 계산하기 위한 세션 생성
initialize_op = tf.global_variables_initializer() # 변수의 초기화 진행
sess.run(initialize_op) # 계산 그래프의 실행
```

#### Placeholder

__Placeholder__ (플레이스홀더) 란 계산 그래프에 데이터가 투입되는 위치를 나타내는 것이다. __Session__ 의 __feed_dict__ 인자를 통해서 플레이스홀더에 데이터를 투입한다. 계산 그래프에 플레이스홀더를 넣기 위해서는 플레이스홀더에 대해 _하나 이상_ 의 연산을 수행해야 한다.

```python
### 플레이스홀더 선언 및 사용 예제
sess = tf.Session() # 계산 그래프의 초기화
x = tf.placeholder(tf.float32, shape=[2,2]) # 플레이스홀더 x 정의
y = tf.identity(x) # 플레이스홀더 x를 그대로 반환하는 x에 대한 항등 연산 y
x_vals = np.random.rand(2,2) # 플레이스 홀더 x에 투입할 데이터 생성
sess.run(y, feed_dict={x : x_vals}) # 항등 연산 실행
```
위의 예제에서 주의할 것은 마지막 부분의 계산 그래프의 실행에 있어서 플레이스홀더를 그대로 _자체 참조로 반환을 하지 않는다_ 는 점이다. 그래서 항등 연산 y를 정의하고 반환을 진행한 것이다. 아래 그림은 위의 예제에서 초기화된 플레이스홀더의 계산 그래프를 나타낸다. 회색 영역에는 관련 상수와 연산이 표시된다.

![Alt Text][placeholder]

#### Matrix
계산 그래프를 통해 데이터가 어떻게 흘러가는지 이해하기 위해서는 텐서플로우의 행렬 처리 방식을 이해하는 것이 중요하다. 많은 알고리즘에서는 행렬 연산을 사용하는데, 텐서플로우에는 행렬 계산을 위한 연산들이 존재한다.

행렬 생성하는 방법에는 __numpy 배열__ 이나 __중첩 리스트__ 로 _2차원 행렬_ 을 생성하는 것이 가능하다. 혹은 ```diag()``` 함수를 이용하여 _1차원 배열_ 이나 _리스트_ 를 대각 행렬로 만드는 것이 가능하다.

다음 예제는 행렬의 각종 연산에 대하여 다룬 예제 코드이다.

```python
### 행렬 선언 및 각종 연산
identity_matrix = tf.diag([1.0, 1.0, 1.0]) # 대각 행렬
A = tf.truncated_normal([2,3]) # 절단정규분포로부터의 난수값을 반환.
B = tf.fill([2,3], 5.0) # 5.0 으로 채워진 행렬
C = tf.random_uniform([3,2]) # 3X2 Random으로 만들어진 행렬
D = tf.convert_to_tensor(np.array([[1.,2.,3.,],[-3.,-7.,-1.],[0.,5.,-2.]])) # 3X3 으로 각각 숫자가 대입된 행렬
A+B # 행렬 덧셈(뺄셈은 A-B를 대입하면 된다.)
tf.matmul(B, identity_matrix) # 행렬 곱셈 : 행렬을 전치할 것인지, 행렬이 희소 행렬인지를 인자를 통해 지정 가능
tf.transpose(C) #행렬 인자의 전치
tf.matrix_determinant(D) # 행렬식의 계산
tf.matrix_inverse(D) # 역행렬
tf.cholesky(identity_matrix) # Cholesky Decomposition
tf.self_adjoint_eig(D) # 행렬 고유값 및 고유 벡터 : 첫 행 - 고유값 / 다음 행 - 고유 벡터
```
여기서 __Cholesky decomposition__ (숄레스키 분해)은 _Hermitian matrix_ (에르미트 행렬), _Positive-definite matrix_ (양의 정부호행렬)의 분해에서 사용되는데, 이 결과로 _하삼각 행렬_ 과 _하삼각행렬의 켤레전치 행렬_ 의 곱으로 표현된다.

#### 계산 그래프의 연산 방법

텐서플로우에서 계산 그래프의 연산의 과정은 다음과 같다.
1. 계산 그래프의 __데이터 생성__
2. 계산 그래프의 __연산 생성__
3. 그래프에 __데이터를 투입__ 후 결과 출력

다음은 한 개와 여러 개의 연산을 넣은 계산 그래프를 나타낸 예제이다.

```python
### 단일 연산이 들어간 계산 그래프
# 1. 계산 그래프의 데이터 생성
x_vals = np.array([1.,3.,5.,7.,9.])
x_data = tf.placeholder(tf.float32)
m_const = tf.constant(3.)
# 2. 계산 그래프의 연산 생성
my_product = tf.multiply(x_data, m_const) # x_data에 데이터를 투입하고 상수 m_const와 multiply 연산을 수행
# 3. 계산 그래프에 데이터를 투입한 후 결과 출력
for x_val in x_vals:
    print(sess.run(my_product, feed_dict={x_data:x_val})) # 결과 : 3.0 9.0 15.0 21.0 27.0

### 여러 개의 연산이 들어간 계산 그래프
my_array = np.array([[1.,3.,5.,7.,9.],
                     [-2.,0.,2.,4.,6.],
                     [-6.,-3.,0.,3.,6]]) # 3X5 행렬 데이터 생성
x_vals = np.array([my_array, my_array+1]) # 행렬 my_array와 my_array 행렬에 각각 1을 더한 행렬 생성
x_data = tf.placeholder(tf.float32, shape=(3,5)) # 플레이스홀더 생성

m1 = tf.constant([[1.],[0.],[-1.],[2.],[4.]]) # 행렬 곱셈 상수 생성1
m2 = tf.constant([[2.]]) # 행렬 곱셈 상수 생성2
a1 = tf.constant([[10.]]) # 행렬 덧셈 상수 생성

prod1 = tf.matmul(x_data,m1) # 행렬 곱셈1
prod2 = tf.matmul(prod1, m2) # 행렬 곱셈2
add1 = tf.add(prod2,a1) # 행렬 덧셈

# 계산 결과 출력[[102.] [66.] [58.]]-my_array  [[114.] [78.] [70.]]-my_array+1 계산 결과
for x_val in x_vals:
    print(sess.run(add1, feed_dict={x_data:x_val}))
```
여러 개의 연산이 들어가 있는 계산 그래프의 경우에는 마지막 연산을 결과로 출력하면 이전의 연산이 모두 이어지는 것을 알 수 있다. 각각의 예제의 계산 그래프는 다음과 같이 시각화 된다. (왼쪽 : 여러 개의 연산 적용 그래프, 오른쪽 : 한 개의 연산 적용)

![Alt Text][cal_graph]

만약에 차원이 다양하거나 알 수 없는 경우를 처리하기 위해서는 ```None``` 으로 지정을 한다. 예를 들어 플레이스홀더의 열 크기를 알 수 없는 경우

```python
x_data = tf.placeholder(tf.float32, shape=(3,None))
```
위와 같이 하는데, 행렬의 곱셈 조건이 깨질 수 있지만, 상수를 곱할 때 행의 개수가 동일해야 한다는 조건은 만족하게 된다. 이렇게 함으로써 데이터를 동적으로 조건에 맞게 생성이 가능하고, 또한 데이터를 그래프에 투입할 때 ```x_data``` 의 형태를 조절할 수도 있다.

#### 비용 함수(손실 함수) 구현

__Loss Function__ (비용 함수)는 머신러닝에서 모델의 결과 값과 실제 값 사이의 _거리_ 를 측정하는 함수이다. 머신 러닝의 알고리즘을 최적화하기 위해서는 모델의 결과를 평가해야 하는데, 예측 결과가 희망하는 결과에 비해 얼마나 좋은지, 나쁜지를 알려주는 척도가 된다. 일반적으로 __예측 값과 대상 값을 비교__ 하여 둘 사이의 차이를 수치화하여 표현한다. 비용 함수의 대표적인 알고리즘과 그의 특징은 다음과 같다.

| Loss Function | 사용 | 장점 | 단점
|:-----:|:----:|:-----|:-----|
|L2 Norm|회귀|좀 더 안정적|덜 견고함|
|L1 Norm|회귀|좀 더 견고함|덜 안정적|
|Pseudo-Huber|회귀|좀 더 견고하고 안정적|매개변수가 하나 추가|
|Hinge|분류|SVM에서 사용할 최대 여백을 생성|이상치가 비용에 무제한적으로 영향을 줄 수 잇음|
|Cross Entropy|분류|좀 더 안정적|비용이 무제한적으로 영향을 받음. 덜 견고함|

__Norm__ 은 _벡터의 길이나 크기_ 를 측정하는 함수이다. Norm이 측정한 벡터의 크기는 원점에서 벡터 좌표까지의 거리이다. 아래의 수식은 Norm의 수식이다.

![Alt Text][Norm]

p는 Lorm의 차수를 의미하는데, __p가 1이면 L1 Norm, 2이면 L2 Norm__ 이다. 또한 n은 대상 벡터의 요소의 수를 의미한다. Norm은 각 요소별로 요소 절대값을 p번 곱한 값의 합을 q 제곱근한 값이다.

__L2 Norm 비용 함수__ 는 _유클리드 비용 함수_ 라고도 하는데, 대상 값과의 거리 제곱 값이다. L2 Norm은 대상 값 근처에서 기울기가 커지기 때문에 대상 값에 다가갈수록 알고리즘의 수렴 속도를 늦출 수 있다는 장점이 있다. L2 Norm에 대한 수식은 다음과 같다.

![Alt Text][l2_norm]

__L1 Norm 비용 함수__ 는 _절대 비용 함수_ 라고도 한다. 대상 값과의 차이를 제곱하는 대신 절댓값을 취한다는 것에서 L2 Norm과 차이를 보이고, 이에 따라서 값 차이의 크기에 반응하는 정도가 L2 Norm보다 적기 때문에, 이상치 처리에는 L1 Norm이 더 선호된다. 하지만 L1 Norm은 대상 값 위치에서 매끄럽지 않은 모양을 띄기 때문에, 알고리즘 수렴이 잘 안될 수 있다는 단점이 있어서 사용에 주의해아한다. L1 Norm에 대한 수식은 다음과 같다.

![Alt Text][l1_norm]

__Pseudo-Huber 비용 함수__는 Huber 비용 함수를 연속적인 매끄러운 함수로 근사한 것이다. _대상 값 근처에서 볼록_ 하고, _대상 값에서 먼 곳에서는 기울기가 급하지 않은 형태_ 를 가지고 있어서 L1 Norm과 L2 Norm의 장점만을 취한 함수이다. 함수 형태는 경사도를 결정하는 _delta_ (델타)에 따라 결정된다. Pseudo-Huber 비용 함수의 수식은 다음과 같다.

![Alt Text][huber]

다음 예제는 _L2 Norm, L1 Norm, Pseudo-Huber_ 비용 함수들을 구현한 예제이다. ```x_vals``` 의 값을 -1과 1 사이의 500개의 숫자로 _예측 값_ 이라고 가정하고, ```target``` 을 _대상 값_ 이라고 지정하였을 때 각각 비용 함수들에 대한 결과이다. 결과는 아래의 그래프로 나타낸다. 위 세 개의 비용 함수들은 연속 종속 변수를 예측하는 __회귀 비용 함수__ 이다.

```python
### 회귀에 대한 비용 함수

x_vals = tf.linspace(-1.,1.,500) # -1부터 1사이의 값 500개 생성 - 예측 값으로 가정
target = tf.constant(0.) # 대상 값

#### L2 Norm
l2_y_vals = tf.square(target - x_vals) # square() 함수는 제곱 값을 구하는 함수이다.
l2_y_out = sess.run(l2_y_vals)

#### L1 Norm
l1_y_vals = tf.abs(target - x_vals) # abs() 함수는 절댓값을 구하는 함수이다.
l1_y_out = sess.run(l1_y_vals)

#### Pseudo-Huber
delta1 = tf.constant(0.25) # delta 값을 0.25로 지정하였을 때
phuber1_y_vals = tf.multiply(tf.square(delta1), tf.sqrt(1. + tf.square((target - x_vals)/delta1)) - 1.) # Pseudo-Huber 비용 함수
phuber1_y_out = sess.run(phuber1_y_vals)
delta2 = tf.constant(5.) # delta 값을 5.0으로 지정하였을 때
phuber2_y_vals = tf.multiply(tf.square(delta2), tf.sqrt(1. + tf.square((target - x_vals)/delta2)) - 1.) # Pseudo-Huber 비용 함수
phuber2_y_out = sess.run(phuber2_y_vals)

#### 회귀 비용 함수 그래프
x_array = sess.run(x_vals)
plt.plot(x_array, l2_y_out, 'b-', label='L2 Norm') # L2 Norm
plt.plot(x_array, l1_y_out, 'r-', label='L1 Norm') # L1 Norm
plt.plot(x_array, phuber1_y_out, 'k-.', label='Pseudo-Huber 0.25') # Delta 값이 0.25인 Pseudo-Huber
plt.plot(x_array, phuber2_y_out, 'g-', label='Pseudo-Huber 5.0') # Delta 값이 5.0인 Pseudo-Huber
plt.ylim(-0.2, 0.6) # y 값 범위
plt.legend(loc='lower right', prop={'size': 8}) # 그래프 설명 위치 및 사이즈 지정
## * 그래프 위치 지정 : best, upper/center/lower, right/center/left
plt.show()
```

![Alt Text][regression_loss_func]

__ Hinge 비용 함수__ 는 _Support Vector Machines(SVM)_ 에서 주로 사용하지만, 신경망에서도 사용이 가능한 함수이다. Hinge 비용 함수는 학습데이터 각각의 범주를 구분하면서 데이터와의 거리가 가장 먼 __decision boundary__ (결정경계)를 찾기 위해 고안된 비용 함수이다. 여기서 결정경계란 주어진 데이터에 대한 종류를 잘 구분할 수 있는 경계라는 의미이다. 두 분류 대상인 1과 -1에 대한 비용을 계산하는데, 대상 값이 1이라고 한다면, 예측 값이 1에 가까워질수록 비용 함수 값이 작아진다. Hinge 비용 함수에 대한 수식은 다음과 같다.

![Alt Text][hinge]

여기서 __y'__ 는 모델의 예측 값(스칼라), __y__ 는 학습 데이터의 실제 값(-1 또는 1)을 말한다. 1에 가까워질수록 비용 함수의 값이 작아진다고 하였으니 그래프로 표현하면 다음과 같다.

![Alt Text][hinge_graph]

__Cross Entropy 비용 함수__ 는 __Logistic 비용 함수__ 라고도 부르며, _0과 1 두 분류를 예측_ 할 때 사용한다. Cross Entropy 비용 함수는 정보 이론에서 정보 압축 코드에 대한 생각에서 비롯된 함수이다. 0과 1사이의 실수 값으로 주어지는 예측 결과와 실제 분류 값(0또는 1) 사이의 거리를 측정해야 한다. Cross Entropy에 대한 수식은 다음과 같다.

![Alt Text][cross_entropy]

y는 모델이 예측한 확률 분포이고 y'는 학습 데이터의 분포(one-hot Vector : 하나의 차원만 1이고 나머지 차원은 0으로 채워진 벡터)이다. 여기서 오차가 크면 클수록 _수렴 속도가 빠르고_ , 반대로 오차가 적다면 수렴 속도가 느려져 _발산을 방지_ 할 수 있다.

__Softmax Cross Entropy 비용 함수__ 는 _정규화되지 않은 출력 값_ 을 대상으로 하는데, 여럿이 아닌 _하나의 분류 대상_ 에 대한 비용을 측정할 때 사용한다. 그렇기 때문에 __softmax 함수__ 를 이용해 결과 값을 확률 분포로 변환하고 실제 확률 분포와 비교하는 방식으로 비용을 계산한다. softmax 함수에 대한 수식은 다음과 같다.

![Alt Text][softmax]

[Softmax Cross Entropy](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)

다음 예제는 __Hinge, Cross Entropy__ 비용 함수에 대한 코드와 그래프로 표현한 예제이다. 예측 값을 -3과 5 사이의 500개의 값을 만들고, ```target```과 ```targets``` 라는 대상 값을 사용하여 진행하였다. 또한, __Softmax Cross Entropy__ 비용 함수에 대한 예제도 구현하였다. 위의 비용 함수들은 분류 예측 결과에 대한 비용을 평가하는 __분류 비용 함수__ 이다.

```python
### 분류 예측 결과에 대한 비용 평가 함수
x_vals = tf.linspace(-3., 5., 500) # -3부터 5까지의 값 500개 생성 - 예측 값 재정의
target = tf.constant(1.) # 대상 값
targets = tf.fill([500,], 1.) # 대상 값 - 동일한(1.0) 으로 채워진 값

#### Hinge
hinge_y_vals = tf.maximum(0., 1. - tf.multiply(target, x_vals)) # Hinge 비용 함수
hinge_y_out = sess.run(hinge_y_vals)

#### Cross Entropy
xentropy_y_vals = - (tf.multiply(target, tf.log(x_vals)) + tf.multiply((1. - target), tf.log(1. - x_vals))) # Cross Entropy 비용 함수(target : y, x_vals : z 값)
xentropy_y_out = sess.run(xentropy_y_vals)

#### 분류 비용 함수 그래프
x_array = sess.run(x_vals)
plt.plot(x_array, hinge_y_out, 'b-', label='Hinge Loss')
plt.plot(x_array, xentropy_y_out, 'r--', label='Cross Entropy Loss')
plt.ylim(-1.5, 3)
plt.legend(loc='lower left', prop={'size': 8})
plt.show()

#### Softmax Cross Entropy
unscaled_logits = tf.constant([[1., -3., 10.]]) # 정규화가 이루어지지 않은 출력값
target_dist = tf.constant([[0.1, 0.02, 0.88]]) # 실제 확률 분포
softmax_xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=unscaled_logits, labels=target_dist) # Softmax Cross Entropy
print(sess.run(softmax_xentropy)) # 결과값 : [1.1601256]
```

![Alt Text][classification_loss_func]

[Tensorflow_logo]:https://imgur.com/IB5SYiQ.jpg
[tf_algo]:https://imgur.com/Nq3ifOv.png
[placeholder]:https://imgur.com/AZshdL5.png
[cal_graph]:https://imgur.com/d6u3al7.png
[Norm]:https://imgur.com/SbDCgcb.png
[l2_norm]:https://imgur.com/Oi4RO33.png
[l1_norm]:https://imgur.com/8CuOPC7.png
[huber]:https://imgur.com/xGUvTnc.png
[regression_loss_func]:https://imgur.com/AtBwP3u.png
[hinge]:https://imgur.com/BAedxP6.png
[hinge_graph]:https://imgur.com/xcqIKLz.png
[cross_entropy]:https://imgur.com/LWcaJic.png
[classification_loss_func]:https://imgur.com/2XDIgZ4.png
[softmax]:https://imgur.com/B1wCxOK.png
