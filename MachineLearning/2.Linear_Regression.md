## 2. Linear Regression (선형 회귀)
> Linear Regression의 개념과 튜토리얼을 제공합니다.

### Task
Machine learing 기본개념

### index
- Regression?
- Linear Regression
- How to minimize cost
- Multivariable linear regression
- Tutorial

#### Regression이란?
![define reg][define regression]
> 출처 :네이버사전

이 사전적 의미에서도 통계학에서의 의미가 머신러닝에서 regression과 가장유사하다고 생각한다. 이 의미를 생각하며 머신러닝에서의 regression을 파악해보자.

Regression 을 파악하기 앞서 Supervised learning에 대한 개념을 간단하게 살펴본다.

Machine Learning은 프로그램을 학습시키는 분야이다. 프로그램을 학습시키는 방법은 Supervised Learning과 Unsupervised Learning이 있다. Supervised learing은 학습시키고자 하는 label이 있는 예시(데이터)를 제공해 학습시키는 방법이다.  여기서 label이 있는 데이터를 training data set 이라 부른다.

![Training Data][training data]
> Training data set 예시 - http://hunkim.github.io/ml/

반대로 Unsupervised learning은 label이 없는 데이터를 학습시키는 종류이다. 즉, 데이터를 보고 스스로 학습하는 하는것을 말한다.

Supervised learning은 다음과 같은 모델로 나눌 수 있다.
- regression  _ex) 공부 시간에 따른 성적 예상(0 ~ 100)_
- binary classification  _ex) 공부 시간에 따른 시험 합격/불합격(P or F)_
- multi-label classification  _ex)  공부 시간에 따른 학점(A,B,C,D...F)_

regression모델은 공부 시간에 따른 성적 데이터로 학습시켰을 때 특정 점수를 예측하는 프로그램을 예로 들 수 있다.
위에 적어놓은 예시와 마찬가지로 binary classification은 결과를 PASS 혹은 FAIL처럼 분류할 때,  multi-label classification은 학점 예측 프로그램처럼  여러 분류항목 중에 예측하는 것을 예로 들 수 있다.
![Regression][regression model]
> regression모델을 사용한 프로그램 예시 

**즉 Regression(회귀분석)은 학습시키기위한 데이터인 training data set들로 학습함으로써 데이터 관계를 파악하는 방법이고, 이렇게 생성된 모델을 기반으로 위의 예시와 같이 점수 예측 프로그램에 사용될 수 있는 것이다.**

이제 regression에 대한 의미가 어느정도 파악되었으니 Linear regression에 대해 알아보도록 한다.

#### Linear Regression?
선형회귀분석(linear regression)은 독립변수 x, 상수항 b와 종속변수 y사이의 관계를 모델링하는 방법이다. 즉 선형회귀분석은 주어진 x,y값(training data set)이 linear한 모델을 가질 것이라 가설을 세우고 가장 잘 맞는 linear한 선을 찾는 것이라 할 수 있다. 이 가장 적합한 선을 찾는 것이 학습이라 할 수 있다. 이 선형회귀분석은 두 변수 사이의 관계일 경우 단순회귀라고 하며 여러 개의 변수를 다루는 다중회귀도 있다.

다음 그림의 왼쪽은 주어진 training data set이고 오른쪽은 그것을 그래프로 나타낸 것이다.
![linear ex1][linear ex1]

 주어진 training data set은 이미 선형의 모델을 가질 것으로 예상할 수 있지만, 실제 데이터는 그렇지 않다. 그러므로 주어진 값들이 선형의 모델을 가질 것이라 가설을 세우고 가장 적합한 linear한 선을 찾는다.
![linear ex2][linear ex2]

위와 같이 학습과정에서 각기 다른 선형모델들이 생긴다. 이는 다음과 같은 수식으로 나타낼 수 있다.

**__H(x) = Wx + b__**

이 수식은 W와 b 값에 따라 선의 모양이 달라진다. 그러므로 linear regression모델로 학습하는 것은 가장 적절한 W,b값을 구하는 것과 같은 것이다.

그렇다면 어떻게 가장 적합한 W,b값을 구하는지 알아보도록 한다.
 ![linear ex2][linear ex3]

 위 그림에서의 파란 선은 추정된 선이다. 이 선이 적합한지 알아보기 위해선 실제 값과 얼마나 차이나는지를 비교하면 될 것이다. 이렇게 실제 값과 추정된 선의 차를 비교하는 것을 linear regression에서는 cost function(비용함수)라고 한다. cost function을 통해 적합한 모델을 찾는다. linear regression에서 cost function은 다음과 같이 나타낸다.
  ![linear ex2][linear ex4]
  
두 값의 차의 제곱을 한 이유는 값을 양수로 나타내기 위해서이고, 이렇게 주어진 데이터에서 각각 차이를 제곱한 후 모두 더하고 데이터의 수만큼 나눠준다. 
![linear ex2][linear ex5]
이러한 cost(W,b)을 최소화 하는 W,b를 구하는 것이 linear regression의 학습이 된다.


-------------------------------
[training data]:https://i.imgur.com/MoqPzMf.png
[regression model]:https://i.imgur.com/QJyDQ4G.png
[define regression]:https://i.imgur.com/FdJmzhv.png
[linear ex1]:https://i.imgur.com/bVDYH1l.png
[linear ex2]:https://i.imgur.com/tO479H6.png
[linear ex3]:https://i.imgur.com/fQXeeC9.png
[lienar ex4]:https://i.imgur.com/aUxUzZg.png
[linear ex5]:https://i.imgur.com/IJ3p5Hd.png
