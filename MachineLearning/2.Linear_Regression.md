## 2. Linear Regression (선형 회귀)
> Linear Regression의 개념과 튜토리얼을 제공합니다.

### Task
Machine learing 기본개념

### index
1. Regression?
2. Linear Regression
3. How to minimize cost
4. Tutorial

### 1. Regression이란?

![define reg][define regression]
> 출처 :네이버사전

이 사전적 의미에서도 통계학에서의 의미가 머신러닝에서 regression과 가장유사하다고 생각한다. 이 의미를 생각하며 머신러닝에서의 regression을 파악해보자.

Regression 을 파악하기 앞서 Supervised learning에 대한 개념을 간단하게 살펴본다.

Machine Learning은 프로그램을 학습시키는 분야이다. 프로그램을 학습시키는 방법은 Supervised Learning과 Unsupervised Learning이 있다. Supervised learing은 학습시키고자 하는 label이 있는 예시(데이터)를 제공해 학습시키는 방법이다.  여기서 label이 있는 데이터를 training data set 이라 부른다.

![Training Data][training data]
> Training data set 예시 - http://hunkim.github.io/ml/

반대로 Unsupervised learning은 label이 없는 데이터를 학습시키는 종류이다. 즉, 데이터를 보고 스스로 학습하는 하는것을 말한다.

Supervised learning은 다음과 같은 모델로 나눌 수 있다.
- regression  _ex) 공부 시간에 따른 성적 예상(0 ~ 100)_
- binary classification  _ex) 공부 시간에 따른 시험 합격/불합격(P or F)_
- multi-label classification  _ex)  공부 시간에 따른 학점(A,B,C,D...F)_

regression모델은 공부 시간에 따른 성적 데이터로 학습시켰을 때 특정 점수를 예측하는 프로그램을 예로 들 수 있다.
위에 적어놓은 예시와 마찬가지로 binary classification은 결과를 PASS 혹은 FAIL처럼 분류할 때,  multi-label classification은 학점 예측 프로그램처럼  여러 분류항목 중에 예측하는 것을 예로 들 수 있다.

![Regression][regression model]
> regression모델을 사용한 프로그램 예시 

**즉 Regression(회귀분석)은 학습시키기위한 데이터인 training data set들로 학습함으로써 데이터 관계를 파악하는 방법이고, 이렇게 생성된 모델을 기반으로 위의 예시와 같이 점수 예측 프로그램에 사용될 수 있는 것이다.**

이제 regression에 대한 의미가 어느정도 파악되었으니 Linear regression에 대해 알아보도록 한다.

### 2. Linear Regression?

선형회귀분석(linear regression)은 독립변수 x, 상수항 b와 종속변수 y사이의 관계를 모델링하는 방법이다. 즉 선형회귀분석은 주어진 x,y값(training data set)이 linear한 모델을 가질 것이라 가설을 세우고 가장 잘 맞는 linear한 선을 찾는 것이라 할 수 있다. 이 가장 적합한 선을 찾는 것이 학습이라 할 수 있다. 이 선형회귀분석은 두 변수 사이의 관계일 경우 단순회귀라고 하며 여러 개의 변수를 다루는 다중회귀도 있다.

다음 그림의 왼쪽은 주어진 training data set이고 오른쪽은 그것을 그래프로 나타낸 것이다.

![linear ex1][linear ex1]

 주어진 training data set은 이미 선형의 모델을 가질 것으로 예상할 수 있지만, 실제 데이터는 그렇지 않다. 그러므로 주어진 값들이 선형의 모델을 가질 것이라 가설을 세우고 가장 적합한 linear한 선을 찾는다.
 
![linear ex2][linear ex2]

위와 같이 학습과정에서 각기 다른 선형모델들이 생긴다. 이는 다음과 같은 수식으로 나타낼 수 있다.

**__H(x) = Wx + b__**

이 수식은 W와 b 값에 따라 선의 모양이 달라진다. 그러므로 linear regression모델로 학습하는 것은 가장 적절한 W,b값을 구하는 것과 같은 것이다.

그렇다면 어떻게 가장 적합한 W,b값을 구하는지 알아보도록 한다.

![linear ex3][linear ex3]

 위 그림에서의 파란 선은 추정된 선이다. 이 선이 적합한지 알아보기 위해선 실제 값과 얼마나 차이나는지를 비교하면 될 것이다. 이렇게 실제 값과 추정된 선의 차를 비교하는 것을 linear regression에서는 cost function(비용함수)라고 한다. cost function을 통해 적합한 모델을 찾는다. linear regression에서 cost function은 다음과 같이 나타낸다.

![linear ex4][linear ex4]

두 값의 차의 제곱을 한 이유는 값을 양수로 나타내기 위해서이고, 이렇게 주어진 데이터에서 각각 차이를 제곱한 후 모두 더하고 데이터의 수만큼 나눠준다. 이는 다음과 같은 코드로 나타낸다.
```python
loss = tf.reduce_mean(tf.square(y - y_data))
```

위에서 말했듯이 두 값의 차를 tf.square를 통해 제곱하고 tf.reduce_mean을 통해 평균을 계산한다.

![linear ex5][linear ex5]

이러한 cost(W,b)을 최소화 하는 W,b를 구하는 것이 linear regression의 학습이 된다.

### 3. How to minimize cost?

간단하게 cost를 최소화하기 위해 위에서 정의한 H(x)를 __H(x) = Wx__ 로 나타낼 수 있다. b 값을 제거한 식이다. 따라서 cost에 대한 식은 다음과 같이 나타낼 수 있다.

![linear ex6][linear ex6]

cost를 W에 대한 이차방정식으로 나타낼 수 있고, 그래프는 다음과 같은 모양을 갖게 된다.

![linear ex7][linear ex7]

그래프를 보면 x축은 W, y축은 cost(W)값을 나타낸다. 여기서 cost(W)값이 가장 작을 때의 W값을 알 수 있어야 안다면 가장 적절한 H(x)를 알게 된다. 그래프에서 cost(W)의 최소값은 기울기가 0에 가까운 가장 아래 부분일 것이다. 우리는 이렇게 그래프를 보면 알 수 있지만, 프로그램은 그렇지 않다. 이를 실행하는 **Gradient decent algorithm(경사하강법)** 은 cost function의 최소값을 구하는 좋은 방법이 된다. Gradient decent 알고리즘을 수행하기 위해 텐서플로는 cost function의 기울기를 계산(편미분)한다.

Gradient decent는 일련의 매개변수로 된 함수가 주어지면 초기 시작점에서 함수의 값이 최소화되는 방향으로 매개변수를 변경하는 것을 반복적으로 수행하는 알고리즘이다. 즉 함수의 값 cost(W)가 최소화되는 방향으로 매개변수인 W를 변경하면서 반복적으로 학습하는 것이다. 이러한 알고리즘은 코드로 다음과 같이 사용한다.
```python
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)
```

여기서 0.5를 나타내는 매개변수는 학습 속도이다. 0.5의 학습 속도를 적용한 Gradient decent의 minimize기능을 통해 위에서 정의한 cost(W)를 매개변수로 train노드를 정의해주는 내용이다.
>학습속도(learing rate)는 텐서플로가 각 반복 때마다 얼마나 크게 이동할 것인가 제어한다. 학습속도를 너무 크게 하면 최소값을 지나쳐 버릴 수 있다. 반대로 학습 속도를 너무 작게 하면 최소값에 다다르는 데 많은 반복이 필요하다. 그러므로 적절한 학습 속도를 선택하는 것이 중요하다.

### 4. Tutorial
>이 튜토리얼은 사람의 뇌무게와 몸무게의 data set으로 위에서 공부한 linear regression에 대해 튜토리얼을 진행한다. 튜토리얼을 통해 cost function을 정의해보고 Gradient decent algorithmn을 적용해본다. 결론적으로 튜토리얼에서 사람의 뇌무게와 몸무게가 비례하는 그래프를 그려보고 linear regression 을 통해 학습한 적절한 모델을 찾는다.

#### Reference
- https://github.com/ahastudio/CodingLife/blob/master/20170304/tensorflow/test.ipynb
- http://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html

튜토리얼은 github의 소스코드와 위의 데이터 셋을 제공하는 사이트를 참고하였다.

#### Task
- tensorflow 설치
- pandas 설치(data 구조를 쉽게 사용하고, 분석할 있는 오픈 소스 라이브러리 : dataset을 읽을 때 사용)
- matplotlib 설치(차트를 그릴 때 사용)
- dataset

#### index
- 준비
- 소스코드
- 결과

#### 준비

먼저 python, tensorflow 는 설치되어 있다고 가정하고, pandas와 matplotlib를 설치한다.
```
pip install pandas
pip install matplotlib
```
data set은 다음과 같다.  63개의 데이터가 있다.

![linear ex8][linear ex8]


#### 소스코드

튜토리얼의 소스코드이다. 아래의 소스코드는 필요한 라이브러리를 import하고 같은 폴더에 저장된 dataset csv파일을 불러와 데이터를 training data set을 정의하는 과정이다.
```python
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

### import한 pandas로 csv자료를 읽는다.
df = pd.read_csv('data.csv')

# Sort by X

### 데이터를 body_weight 기준으로 오름차순으로 정렬한다.
### dataset의 갯수가 63개 인데, 마지막 데이터만 차이가 매우 커서 50개로 줄였다.
df = df.sort_values(by='body_weight')
df = df[df.body_weight < 50]


# Data set

### 데이터의 오차(?)를 없애기 위해 0 ~ 1.0사이의 수로 조정한다.
def normalize(x):
    max_x = max(x)
    min_x = min(x)
    return (x - min_x) / (max_x - min_x)


x = normalize(df.body_weight).tolist()
y = normalize(df.brain_weight).tolist()
```

다음은 linear model을 정의하고 cost(loss) function과 Gradient descent를 정의하는 코드이다. 또한 변수를 Initialize한다.
```
# Linear Model

### w,b를 Varaible로 선언하고 y_로 linear model을 정의한다.
w = tf.Variable(tf.truncated_normal([1], stddev=0.1))
b = tf.Variable(tf.truncated_normal([1], stddev=0.1))
y_ = w * x + b

# Loss Function


loss = tf.reduce_mean(tf.square(y - y_))

# Gradient Descent Optimization

### learing_rate를 조정함으로써 학습 속도를 개선할 수 있다. 단, 적절한 학습 속도로 조정해야 한다.
learning_rate = 0.3
optimizer = tf.train.GradientDescentOptimizer(learning_rate)

# Initialize

session = tf.InteractiveSession()
tf.global_variables_initializer().run()
```

다음은 위에서 정의한 학습노드를 실행시킨다. 매 순간 차트를 그리게 정의했다.
```
# Training

###
def draw(index):
    print(index, session.run(w), session.run(b))
    plt.plot(x, y, '--o')
    plt.plot(x, session.run(y_), '--o')
    plt.show()


draw(0)

for i in range(100):
    optimizer.minimize(loss).run()
    draw(i + 1)
```

#### 결과




-------------------------------
[training data]:https://i.imgur.com/MoqPzMf.png
[regression model]:https://i.imgur.com/QJyDQ4G.png
[define regression]:https://i.imgur.com/FdJmzhv.png
[linear ex1]:https://i.imgur.com/bVDYH1l.png
[linear ex2]:https://i.imgur.com/tO479H6.png
[linear ex3]:https://i.imgur.com/fQXeeC9.png
[linear ex4]:https://i.imgur.com/aUxUzZg.png
[linear ex5]:https://i.imgur.com/IJ3p5Hd.png
[linear ex6]:https://i.imgur.com/SkAVaw7.png
[linear ex7]:https://i.imgur.com/cpoyn86.png
[linear ex8]:https://i.imgur.com/nGCLEa1.png
