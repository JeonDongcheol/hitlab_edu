## 4. Neural Network (신경망)
> 이 단원에서는 신경망의 개념과 logistic regression에 대해 학습한다.

### Task


### index
1. 딥러닝(Deep learning)
2. Logistic (regression) Classification
3. Softmax regression(Multitional Logistic regression)
4. Tutorial

### 1. 딥러닝

**딥러닝**은 신경망 알고리즘을 주로 사용하는 머신 러닝의 한 분야라고 할 수 있다. 딥러닝은 뇌의 신경세포의 구조에서 착안하여 고안된 알고리즘을 사용한다.
![neuron][neuron]

우리 뇌는 비교적 단순한 정보를 처리하는  많은  신경세포들로  이루어져있다. 하나의 신경세포는 다른  신경세포들로부터  정보를 받아  새로운 정보를 생성하며, 생성한 정보는 또 다른  신경세포로 전달된다. 이러한 신경세포들의 특징을  바탕으로  1957년 Flank Rosenblatt이 개발한 퍼셉트론(Perceptron)  알고리즘이 딥 러닝과 많은  머신  러닝  알고리즘의 기초가 되었다.

[퍼셉트론과  신경망에  대해  자세히 알아보기][refer1]

하나의 퍼셉트론은 하나의 신경세포를 인공적으로 모델링한 뉴런이라고 이해하면 쉽다. 뉴런 하나로 입력되는 정보가  X1,X2 두 개라고 할 때 입력 값에 각각 가중치 W1,W2를 곱한 후 더한다. 이 값을 어떤 기준과 비교하여 만족스러운 결과가 나오도록 가중치 W1,W2를 조금씩 조정해나간다. 그 결과값은 다음 뉴런으로 전달된다.

![neuron2][neuron2]

신경망은 여러 개의 뉴런이 하나의 계층을 형성하며, 이런 계층을 다시 여러 개 쌓아 올린다. 이 시스템은 퍼셉트론 알고리즘처럼 준비된 표본 데이터를 입력받아 원하는 출력 데이터가 만들어지도록 연결된 뉴런끼리 얼마만큼 정보를 주고받을지의 가중치를 조절하여 신경망에 있는 모든 뉴런 간의 연결을 최적화시킨다. 즉 가장 적합한 모델로 만들어가는 과정이라고 생각하면 된다. 이를 학습, 훈련이라고하고 학습이 완료된 모델에 데이터를넣어 결과를 얻어내는 것을 추론이라고 한다.

신경망에서 데이터를 입력받는 계층을 입력 계층 , 결과값을 만들어내는 계층을 출력 계층이라고 부른다. 입력 계층과 출력 계층 사이에 하나 이상의 계층이 끼어 있을 때는 이를 은닉 계층이라고 부르고, 이러한 구조를 심층 신경망 (DNN)이라고 부르는데, 대부분의 신경망은 은닉계층이 있다.

![neuron3][neuron3]

### 2. Logistic regression
> 신경망 튜토리얼 전 logistic regression을 배워본다 

앞서 2다원에서 간단하게 classification에 대해서 설명한 적이 있다. 특히 여기서 다룰 (Binary) Classification은 데이터를 0또는 1로 분류하는 모델을 말한다. 예를 들자면 스팸메일 감지 프로그램에서 메일이 스팸메일인지 햄메일인지 분류해주는 것으로 볼 수 있다. 다음과 같이 공부시간에 따른 시험 합격 pass of fail도 적용된다.

![neuron4][neuron4]

이는 linear regression과 비슷하게 한 선을 그어 공부한 시간에 따라 pass와 fail을 분류할 수 있다. 하지만 classification에서 원하는 것은 1과 0 두가지로 결과값을 분류하는 것이다. 이러한 classification에 특성에 알맞게 linear모델을 변환시켜줄 수 있는 함수가 있다. 이를 **logistic function(sigmoid function)** 이라고 한다(또는 활성화함수라고 함). linear모델의 수식이었던 **H(x) = W * X**에 sigmoid를 적용하면 다음과 같은 그래프처럼 0과 1사이의 값으로 나눠지는 형태로 나타낼 수 있게 된다.

![neuron5][neuron5]

이렇게 그려지는 모델을 학습하고, 그것이 실제 값과 얼마나 차이나는지 알아보기 위해 linear regression에서 했던 것처럼 cost function을 정의해준다. 

[logistic regression의 cost function도출 과정][refer2]

cost function은 다음과 같다.

![neuron6][neuron6]

또 마찬가지로 logistic regression도 Gradient decent 알고리즘을 통해 가장 적합한 모델을 찾아나갈 수 있다. 코드는 다음과 같이  사용하면 된다.
```python
#cost fucntion
cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis)))

#Minimize
a = tf.Varible(0.1) #learing rate
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)
```

### 3. Multinomoial Classification:Softmax Classification
> 분류해야되는 클래스가 여러개일 때 사용하는 Multinomial Classification중 주로 사용되는 Softmax Classification에 대해서 살펴본다.

multinomial classification은 분류항목이 다음과 그림처럼 여러개 일때 적용하는 모델이다. 

![mr1][mr1]

이것을 분류하기위해 위에서 적용했던 것처럼 linear한 선을 기준으로 나눌 수 있고, 이를 sigmoid function을 사용하면 될것이다. 이를 그래프와 행렬로 나타내어 보았다.

![mr2][mr2]

위의 행렬을 보면 알겠지만, 아직 sigmoid가 적용되지 않은 상태이고, 우리가 원하는 '어떤 클래스로 분류될지' 파악하기 어렵다. 여기서 Softmax 활성화 함수를 사용하면 결과값을 0~1값으로 변환시켜주고 이를 더한 값이 1이 되게 만들어줌으로써 확률을 알 수 있게 해준다.

![mr3][mr3]

이렇게 위와 같이  0.7,0.2,0.1과 같은 상황에서 tensorflow **argmax** 메소드를 통해서 최적값을 1.0 나머지를 0.0으로 변환시켜준다. 

이제 신경망에서 Cost function로 주로 사용되는 cross-entropy 방식으로 실제값과의 오차를 측정한다. 식은 다음과 같다.

![mr4][mr4]

```python
logits = tf.matnul(X,W) + b
hypothesis = tf.nn.softmax(logits)

cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))
```

혹은

```python
logits = tf.matnul(X,W) + b
hypothesis = tf.nn.softmax(logits)

cost = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)
cost = tf.reduce_mean(cost)
```
이렇게 사용할 수 있다.

### 4. Tutorial
>이 튜토리얼은 사람의 뇌무게와 몸무게의 data set으로 위에서 공부한 linear regression에 대해 튜토리얼을 진행한다. 튜토리얼을 통해 cost function을 정의해보고 Gradient decent algorithmn을 적용해본다. 결론적으로 튜토리얼에서 사람의 뇌무게와 몸무게가 비례하는 그래프를 그려보고 linear regression 을 통해 학습한 적절한 모델을 찾는다.

#### Reference
- https://github.com/ahastudio/CodingLife/blob/master/20170304/tensorflow/test.ipynb
- http://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html

튜토리얼은 github의 소스코드와 위의 데이터 셋을 제공하는 사이트를 참고하였다.

#### Task
- tensorflow 설치
- pandas 설치(data 구조를 쉽게 사용하고, 분석할 있는 오픈 소스 라이브러리 : dataset을 읽을 때 사용)
- matplotlib 설치(차트를 그릴 때 사용)
- dataset

#### index
- 준비
- 소스코드
- 결과

#### 준비

먼저 python, tensorflow 는 설치되어 있다고 가정하고, pandas와 matplotlib를 설치한다.
```
pip install pandas
pip install matplotlib
```
data set은 다음과 같다.  63개의 데이터가 있다.

![linear ex8][linear ex8]


#### 소스코드

튜토리얼의 소스코드이다. 아래의 소스코드는 필요한 라이브러리를 import하고 같은 폴더에 저장된 dataset csv파일을 불러와 데이터를 training data set을 정의하는 과정이다.
```python
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

### import한 pandas로 csv자료를 읽는다.
df = pd.read_csv('data.csv')

# Sort by X

### 데이터를 body_weight 기준으로 오름차순으로 정렬한다.
### dataset의 갯수가 63개 인데, 마지막 데이터만 차이가 매우 커서 50개로 줄였다.
df = df.sort_values(by='body_weight')
df = df[df.body_weight < 50]


# Data set

### 데이터의 오차(?)를 없애기 위해 0 ~ 1.0사이의 수로 조정한다.
def normalize(x):
    max_x = max(x)
    min_x = min(x)
    return (x - min_x) / (max_x - min_x)


x = normalize(df.body_weight).tolist()
y = normalize(df.brain_weight).tolist()
```

다음은 linear model을 정의하고 cost(loss) function과 Gradient descent를 정의하는 코드이다. 또한 변수를 Initialize한다.
```
# Linear Model

### w,b를 Varaible로 선언하고 y_로 linear model을 정의한다.
w = tf.Variable(tf.truncated_normal([1], stddev=0.1))
b = tf.Variable(tf.truncated_normal([1], stddev=0.1))
y_ = w * x + b

# Loss Function


loss = tf.reduce_mean(tf.square(y - y_))

# Gradient Descent Optimization

### learing_rate를 조정함으로써 학습 속도를 개선할 수 있다. 단, 적절한 학습 속도로 조정해야 한다.
learning_rate = 0.3
optimizer = tf.train.GradientDescentOptimizer(learning_rate)

# Initialize

session = tf.InteractiveSession()
tf.global_variables_initializer().run()
```

다음은 위에서 정의한 학습노드를 실행시킨다. 매 순간 차트를 그리게 정의했다.
```
# Training

###
def draw(index):
    print(index, session.run(w), session.run(b), session.run(loss))
    plt.plot(x, y, '--o')
    plt.plot(x, session.run(y_), '--o')
    plt.show()


draw(0)

for i in range(100):
    optimizer.minimize(loss).run()
    draw(i + 1)
```

#### 결과
```python
print(index, session.run(w), session.run(b), session.run(loss))
```

실행의 결과로 아래와 같이 출력된다. 마지막 출력되는 loss(cost)값이 0에 수렴하는 것을 볼 수 있다. 이는 첫번째, 두번째 출력되는 W,b값에 따라 가장 적절한 모델이 학습되고 있음을 나타낸다.

![linear ex10][linear ex10]

차트를 보면 차트에 새롭게 그려지는 y_값에 따른 linear model의 기울기가 dataset에 맞게 커지는 변화를 볼 수 있다.

![linear ex9][linear ex9]




-------------------------------
[neuron]:https://i.imgur.com/3dvvf2b.png
[refer1]:http://neuralnetworksanddeeplearning.com/chap1.html
[neuron2]:https://i.imgur.com/BoyIrcL.png
[neuron3]:https://i.imgur.com/FRfJVnt.png
[neuron4]:https://i.imgur.com/eRdZRCt.png
[neuron5]:https://i.imgur.com/7g3RsPk.png
[refer2]:https://www.youtube.com/watch?v=6vzchGYEJBc
[neuron6]:https://i.imgur.com/EvxcGHn.png
[mr1]:https://i.imgur.com/acSv3VB.png
[mr2]:https://i.imgur.com/Bgv4gph.png
[mr3]:https://i.imgur.com/qRPYCX3.png
[mr4]:https://i.imgur.com/S2oDuAd.png
