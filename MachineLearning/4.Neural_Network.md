## 4. Neural Network (신경망)
> 이 단원에서는 신경망의 개념과 logistic regression에 대해 학습한다.

### Task


### index
1. 딥러닝(Deep learning)
2. Logistic (regression) Classification
3. Softmax regression(Multitional Logistic regression)
4. Tutorial

### 1. 딥러닝

**딥러닝**은 신경망 알고리즘을 주로 사용하는 머신 러닝의 한 분야라고 할 수 있다. 딥러닝은 뇌의 신경세포의 구조에서 착안하여 고안된 알고리즘을 사용한다.
![neuron][neuron]

우리 뇌는 비교적 단순한 정보를 처리하는  많은  신경세포들로  이루어져있다. 하나의 신경세포는 다른  신경세포들로부터  정보를 받아  새로운 정보를 생성하며, 생성한 정보는 또 다른  신경세포로 전달된다. 이러한 신경세포들의 특징을  바탕으로  1957년 Flank Rosenblatt이 개발한 퍼셉트론(Perceptron)  알고리즘이 딥 러닝과 많은  머신  러닝  알고리즘의 기초가 되었다.

[퍼셉트론과  신경망에  대해  자세히 알아보기][refer1]

하나의 퍼셉트론은 하나의 신경세포를 인공적으로 모델링한 뉴런이라고 이해하면 쉽다. 뉴런 하나로 입력되는 정보가  X1,X2 두 개라고 할 때 입력 값에 각각 가중치 W1,W2를 곱한 후 더한다. 이 값을 어떤 기준과 비교하여 만족스러운 결과가 나오도록 가중치 W1,W2를 조금씩 조정해나간다. 그 결과값은 다음 뉴런으로 전달된다.

![neuron2][neuron2]

신경망은 여러 개의 뉴런이 하나의 계층을 형성하며, 이런 계층을 다시 여러 개 쌓아 올린다. 이 시스템은 퍼셉트론 알고리즘처럼 준비된 표본 데이터를 입력받아 원하는 출력 데이터가 만들어지도록 연결된 뉴런끼리 얼마만큼 정보를 주고받을지의 가중치를 조절하여 신경망에 있는 모든 뉴런 간의 연결을 최적화시킨다. 즉 가장 적합한 모델로 만들어가는 과정이라고 생각하면 된다. 이를 학습, 훈련이라고하고 학습이 완료된 모델에 데이터를넣어 결과를 얻어내는 것을 추론이라고 한다.

신경망에서 데이터를 입력받는 계층을 입력 계층 , 결과값을 만들어내는 계층을 출력 계층이라고 부른다. 입력 계층과 출력 계층 사이에 하나 이상의 계층이 끼어 있을 때는 이를 은닉 계층이라고 부르고, 이러한 구조를 심층 신경망 (DNN)이라고 부르는데, 대부분의 신경망은 은닉계층이 있다.

![neuron3][neuron3]

### 2. Logistic regression
> 신경망 튜토리얼 전 logistic regression을 배워본다 

앞서 2다원에서 간단하게 classification에 대해서 설명한 적이 있다. 특히 여기서 다룰 (Binary) Classification은 데이터를 0또는 1로 분류하는 모델을 말한다. 예를 들자면 스팸메일 감지 프로그램에서 메일이 스팸메일인지 햄메일인지 분류해주는 것으로 볼 수 있다. 다음과 같이 공부시간에 따른 시험 합격 pass of fail도 적용된다.

![neuron4][neuron4]

이는 linear regression과 비슷하게 한 선을 그어 공부한 시간에 따라 pass와 fail을 분류할 수 있다. 하지만 classification에서 원하는 것은 1과 0 두가지로 결과값을 분류하는 것이다. 이러한 classification에 특성에 알맞게 linear모델을 변환시켜줄 수 있는 함수가 있다. 이를 **logistic function(sigmoid function)** 이라고 한다(또는 활성화함수라고 함). linear모델의 수식이었던 **H(x) = W * X**에 sigmoid를 적용하면 다음과 같은 그래프처럼 0과 1사이의 값으로 나눠지는 형태로 나타낼 수 있게 된다.

![neuron5][neuron5]

이렇게 그려지는 모델을 학습하고, 그것이 실제 값과 얼마나 차이나는지 알아보기 위해 linear regression에서 했던 것처럼 cost function을 정의해준다. 

[logistic regression의 cost function도출 과정][refer2]

cost function은 다음과 같다.

![neuron6][neuron6]

또 마찬가지로 logistic regression도 Gradient decent 알고리즘을 통해 가장 적합한 모델을 찾아나갈 수 있다. 코드는 다음과 같이  사용하면 된다.
```python
#cost fucntion
cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis)))

#Minimize
a = tf.Varible(0.1) #learing rate
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)
```

### 3. Multinomoial Classification:Softmax Classification
> 분류해야되는 클래스가 여러개일 때 사용하는 Multinomial Classification중 주로 사용되는 Softmax Classification에 대해서 살펴본다.

multinomial classification은 분류항목이 다음과 그림처럼 여러개 일때 적용하는 모델이다. 

![mr1][mr1]

이것을 분류하기위해 위에서 적용했던 것처럼 linear한 선을 기준으로 나눌 수 있고, 이를 sigmoid function을 사용하면 될것이다. 이를 그래프와 행렬로 나타내어 보았다.

![mr2][mr2]

위의 행렬을 보면 알겠지만, 아직 sigmoid가 적용되지 않은 상태이고, 우리가 원하는 '어떤 클래스로 분류될지' 파악하기 어렵다. 여기서 Softmax 활성화 함수를 사용하면 결과값을 0~1값으로 변환시켜주고 이를 더한 값이 1이 되게 만들어줌으로써 확률을 알 수 있게 해준다.

![mr3][mr3]

이렇게 위와 같이  0.7,0.2,0.1과 같은 상황에서 tensorflow **argmax** 메소드를 통해서 최적값을 1.0 나머지를 0.0으로 변환시켜준다. 

이제 신경망에서 Cost function로 주로 사용되는 cross-entropy 방식으로 실제값과의 오차를 측정한다. 식은 다음과 같다.

![mr4][mr4]

```python
logits = tf.matnul(X,W) + b
hypothesis = tf.nn.softmax(logits)

cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))
```

혹은

```python
logits = tf.matnul(X,W) + b
hypothesis = tf.nn.softmax(logits)

cost = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)
cost = tf.reduce_mean(cost)
```
이렇게 사용할 수 있다.

### 4. Tutorial
>이 튜토리얼은 tensorflow에서 제공하는 공식 MNIST튜토리얼이다. 손글씨 숫자 이미지를 인식하고 분류하는 실습을 진행해볼 수 있다. 이 튜토리얼을 통해  위에서 학습한 Classification을 적용시켰다.

#### Reference
- https://www.youtube.com/watch?v=ktd5yrki_KA&feature=youtu.be
- https://www.tensorflow.org/tutorials/

#### Task
- tensorflow 설치
- matplotlib 설치
- dataset(tensorflow 제공)

#### index
- 준비
- 소스코드
- 결과

#### 준비

먼저 python, tensorflow 는 설치되어 있다고 가정하고, matplotlib를 설치한다.
```
pip install matplotlib
```

데이터 셋
![tut1][tut1]


#### 소스코드

튜토리얼의 소스코드이다. 아래의 소스코드는 필요한 라이브러리를 import하고 같은 폴더에 저장된 dataset csv파일을 불러와 데이터를 training data set을 정의하는 과정이다.
```python
import tensorflow as tf
import random
import matplotlib.pyplot as plt
tf.set_random_seed(777)  # for reproducibility

from tensorflow.examples.tutorials.mnist import input_data
# Check out https://www.tensorflow.org/get_started/mnist/beginners for
# more information about the mnist dataset
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

nb_classes = 10

# MNIST data image of shape 28 * 28 = 784
X = tf.placeholder(tf.float32, [None, 784])
# 0 - 9 digits recognition = 10 classes
Y = tf.placeholder(tf.float32, [None, nb_classes])

W = tf.Variable(tf.random_normal([784, nb_classes]))
b = tf.Variable(tf.random_normal([nb_classes]))

# Hypothesis (using softmax)
hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)

cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

# Test model
is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))
# Calculate accuracy
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))

# parameters
training_epochs = 15
batch_size = 100

with tf.Session() as sess:
    # Initialize TensorFlow variables
    sess.run(tf.global_variables_initializer())
    # Training cycle
    for epoch in range(training_epochs):
        avg_cost = 0
        total_batch = int(mnist.train.num_examples / batch_size)

        for i in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            c, _ = sess.run([cost, optimizer], feed_dict={
                X: batch_xs, Y: batch_ys})
            avg_cost += c / total_batch

        print('Epoch:', '%04d' % (epoch + 1),
              'cost =', '{:.9f}'.format(avg_cost))

    print("Learning finished")

    # Test the model using test sets
    print("Accuracy: ", accuracy.eval(session=sess, feed_dict={
        X: mnist.test.images, Y: mnist.test.labels}))

    # Get one and predict
    r = random.randint(0, mnist.test.num_examples - 1)
    print("Label: ", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))
    print("Prediction: ", sess.run(
        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))

    plt.imshow(
        mnist.test.images[r:r + 1].reshape(28, 28),
        cmap='Greys',
        interpolation='nearest')
    plt.show()
```

#### 결과

아래의 결과 이미지를 통해 cost가 0에 가까워짐으로써 실제 값과 비슷해진 다는 것을 알 수 있다.
accuracy값 출력을 통해 약 90%의 정확성을 보였고, label과 이미지가 동일함을 보인다.

![tut2][tut2]




-------------------------------
[neuron]:https://i.imgur.com/3dvvf2b.png
[refer1]:http://neuralnetworksanddeeplearning.com/chap1.html
[neuron2]:https://i.imgur.com/BoyIrcL.png
[neuron3]:https://i.imgur.com/FRfJVnt.png
[neuron4]:https://i.imgur.com/eRdZRCt.png
[neuron5]:https://i.imgur.com/7g3RsPk.png
[refer2]:https://www.youtube.com/watch?v=6vzchGYEJBc
[neuron6]:https://i.imgur.com/EvxcGHn.png
[mr1]:https://i.imgur.com/acSv3VB.png
[mr2]:https://i.imgur.com/Bgv4gph.png
[mr3]:https://i.imgur.com/qRPYCX3.png
[mr4]:https://i.imgur.com/S2oDuAd.png
[tut1]:https://i.imgur.com/U0Z2CGc.png
[tut2]:https://i.imgur.com/ckifQhh.png
