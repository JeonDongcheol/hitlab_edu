## 4. Neural Network (신경망)
> 여기서는 신경망의 개념과 logistic regression에 대해 학습한다.

### Task


### index
1. 딥러닝(Deep learning)
2. Logistic (regression) Classification
3. Softmax regression(Multitional Logistic regression)
4. Tutorial

### 1. 딥러닝

**딥러닝**은 신경망 알고리즘을 주로 사용하는 머신 러닝의 한 분야라고 할 수 있다. 딥러닝은 뇌의 신경세포의 구조에서 착안하여 고안된 알고리즘을 사용한다.
![neuron][neuron]

우리 뇌는 비교적 단순한 정보를 처리하는  많은  신경세포들로  이루어져있다. 하나의 신경세포는 다른  신경세포들로부터  정보를 받아  새로운 정보를 생성하며, 생성한 정보는 또 다른  신경세포로 전달된다. 이러한 신경세포들의 특징을  바탕으로  1957년 Flank Rosenblatt이 개발한 퍼셉트론(Perceptron)  알고리즘이 딥 러닝과 많은  머신  러닝  알고리즘의 기초가 되었다.

[퍼셉트론과  신경망에  대해  자세히 알아보기][refer1]

하나의 퍼셉트론은 하나의 신경세포를 인공적으로 모델링한 뉴런이라고 이해하면 쉽다. 뉴런 하나로 입력되는 정보가  X1,X2 두 개라고 할 때 입력 값에 각각 가중치 W1,W2를 곱한 후 더한다. 이 값을 어떤 기준과 비교하여 만족스러운 결과가 나오도록 가중치 W1,W2를 조금씩 조정해나간다. 그 결과값은 다음 뉴런으로 전달된다.

![neuron2][neuron2]

신경망은 여러 개의 뉴런이 하나의 계층을 형성하며, 이런 계층을 다시 여러 개 쌓아 올린다. 이 시스템은 퍼셉트론 알고리즘처럼 준비된 표본 데이터를 입력받아 원하는 출력 데이터가 만들어지도록 연결된 뉴런끼리 얼마만큼 정보를 주고받을지의 가중치를 조절하여 신경망에 있는 모든 뉴런 간의 연결을 최적화시킨다. 즉 가장 적합한 모델로 만들어가는 과정이라고 생각하면 된다. 이를 학습, 훈련이라고하고 학습이 완료된 모델에 데이터를넣어 결과를 얻어내는 것을 추론이라고 한다.

신경망에서 데이터를 입력받는 계층을 입력 계층 , 결과값을 만들어내는 계층을 출력 계층이라고 부른다. 입력 계층과 출력 계층 사이에 하나 이상의 계층이 끼어 있을 때는 이를 은닉 계층이라고 부르고, 이러한 구조를 심층 신경망 (DNN)이라고 부르는데, 대부분의 신경망은 은닉계층이 있다.

![neuron3][neuron3]

### 2. Linear Regression?

선형회귀분석(linear regression)은 독립변수 x, 상수항 b와 종속변수 y사이의 관계를 모델링하는 방법이다. 즉 선형회귀분석은 주어진 x,y값(training data set)이 linear한 모델을 가질 것이라 가설을 세우고 가장 잘 맞는 linear한 선을 찾는 것이라 할 수 있다. 이 가장 적합한 선을 찾는 것이 학습이라 할 수 있다. 이 선형회귀분석은 두 변수 사이의 관계일 경우 단순회귀라고 하며 여러 개의 변수를 다루는 다중회귀도 있다.

다음 그림의 왼쪽은 주어진 training data set이고 오른쪽은 그것을 그래프로 나타낸 것이다.

![linear ex1][linear ex1]

 주어진 training data set은 이미 선형의 모델을 가질 것으로 예상할 수 있지만, 실제 데이터는 그렇지 않다. 그러므로 주어진 값들이 선형의 모델을 가질 것이라 가설을 세우고 가장 적합한 linear한 선을 찾는다.
 
![linear ex2][linear ex2]

위와 같이 학습과정에서 각기 다른 선형모델들이 생긴다. 이는 다음과 같은 수식으로 나타낼 수 있다.

**__H(x) = Wx + b__**

이 수식은 W와 b 값에 따라 선의 모양이 달라진다. 그러므로 linear regression모델로 학습하는 것은 가장 적절한 W,b값을 구하는 것과 같은 것이다.

그렇다면 어떻게 가장 적합한 W,b값을 구하는지 알아보도록 한다.

![linear ex3][linear ex3]

 위 그림에서의 파란 선은 추정된 선이다. 이 선이 적합한지 알아보기 위해선 실제 값과 얼마나 차이나는지를 비교하면 될 것이다. 이렇게 실제 값과 추정된 선의 차를 비교하는 것을 linear regression에서는 cost function(비용함수)라고 한다. cost function을 통해 적합한 모델을 찾는다. linear regression에서 cost function은 다음과 같이 나타낸다.

![linear ex4][linear ex4]

두 값의 차의 제곱을 한 이유는 값을 양수로 나타내기 위해서이고, 이렇게 주어진 데이터에서 각각 차이를 제곱한 후 모두 더하고 데이터의 수만큼 나눠준다. 이는 다음과 같은 코드로 나타낸다.
```python
loss = tf.reduce_mean(tf.square(y - y_data))
```

위에서 말했듯이 두 값의 차를 tf.square를 통해 제곱하고 tf.reduce_mean을 통해 평균을 계산한다.

![linear ex5][linear ex5]

이러한 cost(W,b)을 최소화 하는 W,b를 구하는 것이 linear regression의 학습이 된다.

### 3. How to minimize cost?

간단하게 cost를 최소화하기 위해 위에서 정의한 H(x)를 __H(x) = Wx__ 로 나타낼 수 있다. b 값을 제거한 식이다. 따라서 cost에 대한 식은 다음과 같이 나타낼 수 있다.

![linear ex6][linear ex6]

cost를 W에 대한 이차방정식으로 나타낼 수 있고, 그래프는 다음과 같은 모양을 갖게 된다.

![linear ex7][linear ex7]

그래프를 보면 x축은 W, y축은 cost(W)값을 나타낸다. 여기서 cost(W)값이 가장 작을 때의 W값을 알 수 있어야 안다면 가장 적절한 H(x)를 알게 된다. 그래프에서 cost(W)의 최소값은 기울기가 0에 가까운 가장 아래 부분일 것이다. 우리는 이렇게 그래프를 보면 알 수 있지만, 프로그램은 그렇지 않다. 이를 실행하는 **Gradient decent algorithm(경사하강법)** 은 cost function의 최소값을 구하는 좋은 방법이 된다. Gradient decent 알고리즘을 수행하기 위해 텐서플로는 cost function의 기울기를 계산(편미분)한다.

Gradient decent는 일련의 매개변수로 된 함수가 주어지면 초기 시작점에서 함수의 값이 최소화되는 방향으로 매개변수를 변경하는 것을 반복적으로 수행하는 알고리즘이다. 즉 함수의 값 cost(W)가 최소화되는 방향으로 매개변수인 W를 변경하면서 반복적으로 학습하는 것이다. 이러한 알고리즘은 코드로 다음과 같이 사용한다.
```python
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)
```

여기서 0.5를 나타내는 매개변수는 학습 속도이다. 0.5의 학습 속도를 적용한 Gradient decent의 minimize기능을 통해 위에서 정의한 cost(W)를 매개변수로 train노드를 정의해주는 내용이다.
>학습속도(learing rate)는 텐서플로가 각 반복 때마다 얼마나 크게 이동할 것인가 제어한다. 학습속도를 너무 크게 하면 최소값을 지나쳐 버릴 수 있다. 반대로 학습 속도를 너무 작게 하면 최소값에 다다르는 데 많은 반복이 필요하다. 그러므로 적절한 학습 속도를 선택하는 것이 중요하다.

### 4. Tutorial
>이 튜토리얼은 사람의 뇌무게와 몸무게의 data set으로 위에서 공부한 linear regression에 대해 튜토리얼을 진행한다. 튜토리얼을 통해 cost function을 정의해보고 Gradient decent algorithmn을 적용해본다. 결론적으로 튜토리얼에서 사람의 뇌무게와 몸무게가 비례하는 그래프를 그려보고 linear regression 을 통해 학습한 적절한 모델을 찾는다.

#### Reference
- https://github.com/ahastudio/CodingLife/blob/master/20170304/tensorflow/test.ipynb
- http://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html

튜토리얼은 github의 소스코드와 위의 데이터 셋을 제공하는 사이트를 참고하였다.

#### Task
- tensorflow 설치
- pandas 설치(data 구조를 쉽게 사용하고, 분석할 있는 오픈 소스 라이브러리 : dataset을 읽을 때 사용)
- matplotlib 설치(차트를 그릴 때 사용)
- dataset

#### index
- 준비
- 소스코드
- 결과

#### 준비

먼저 python, tensorflow 는 설치되어 있다고 가정하고, pandas와 matplotlib를 설치한다.
```
pip install pandas
pip install matplotlib
```
data set은 다음과 같다.  63개의 데이터가 있다.

![linear ex8][linear ex8]


#### 소스코드

튜토리얼의 소스코드이다. 아래의 소스코드는 필요한 라이브러리를 import하고 같은 폴더에 저장된 dataset csv파일을 불러와 데이터를 training data set을 정의하는 과정이다.
```python
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

### import한 pandas로 csv자료를 읽는다.
df = pd.read_csv('data.csv')

# Sort by X

### 데이터를 body_weight 기준으로 오름차순으로 정렬한다.
### dataset의 갯수가 63개 인데, 마지막 데이터만 차이가 매우 커서 50개로 줄였다.
df = df.sort_values(by='body_weight')
df = df[df.body_weight < 50]


# Data set

### 데이터의 오차(?)를 없애기 위해 0 ~ 1.0사이의 수로 조정한다.
def normalize(x):
    max_x = max(x)
    min_x = min(x)
    return (x - min_x) / (max_x - min_x)


x = normalize(df.body_weight).tolist()
y = normalize(df.brain_weight).tolist()
```

다음은 linear model을 정의하고 cost(loss) function과 Gradient descent를 정의하는 코드이다. 또한 변수를 Initialize한다.
```
# Linear Model

### w,b를 Varaible로 선언하고 y_로 linear model을 정의한다.
w = tf.Variable(tf.truncated_normal([1], stddev=0.1))
b = tf.Variable(tf.truncated_normal([1], stddev=0.1))
y_ = w * x + b

# Loss Function


loss = tf.reduce_mean(tf.square(y - y_))

# Gradient Descent Optimization

### learing_rate를 조정함으로써 학습 속도를 개선할 수 있다. 단, 적절한 학습 속도로 조정해야 한다.
learning_rate = 0.3
optimizer = tf.train.GradientDescentOptimizer(learning_rate)

# Initialize

session = tf.InteractiveSession()
tf.global_variables_initializer().run()
```

다음은 위에서 정의한 학습노드를 실행시킨다. 매 순간 차트를 그리게 정의했다.
```
# Training

###
def draw(index):
    print(index, session.run(w), session.run(b), session.run(loss))
    plt.plot(x, y, '--o')
    plt.plot(x, session.run(y_), '--o')
    plt.show()


draw(0)

for i in range(100):
    optimizer.minimize(loss).run()
    draw(i + 1)
```

#### 결과
```python
print(index, session.run(w), session.run(b), session.run(loss))
```

실행의 결과로 아래와 같이 출력된다. 마지막 출력되는 loss(cost)값이 0에 수렴하는 것을 볼 수 있다. 이는 첫번째, 두번째 출력되는 W,b값에 따라 가장 적절한 모델이 학습되고 있음을 나타낸다.

![linear ex10][linear ex10]

차트를 보면 차트에 새롭게 그려지는 y_값에 따른 linear model의 기울기가 dataset에 맞게 커지는 변화를 볼 수 있다.

![linear ex9][linear ex9]




-------------------------------
[neuron]:https://i.imgur.com/3dvvf2b.png
[refer1]:http://neuralnetworksanddeeplearning.com/chap1.html
[neuron2]:https://i.imgur.com/BoyIrcL.png
[neuron3]:https://i.imgur.com/FRfJVnt.png
