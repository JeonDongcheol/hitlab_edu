## Clustering(군집화)
> __Unsupervised Learning__ (비지도학습) 방법 중 하나인 Clustering(군집화)에 대해서 학습하는 페이지이다.

#### index

1. [_Clustering_ 이란?](#c1)
2. [_Hierarchical Clustering_ (계층적 군집화)](#c2)
3. [_K-means Clustering_ (K-평균 군집화)](#c3)
3. [_DBSCAN_ ](#c6)
4. [Tutorial : K-평균 군집화](#c4)
5. [미해결 부분](#c5)

### _Clustering_ 이란? <a name="c1"/>

__Clustering__ (군집화)란 _비지도학습_ 방법 중 하나로 비슷한 개체는 한 묶음으로, 비슷하지 않은 개체는 다른 그룹으로 __그룹화__ 하는 방법으로 그룹에 대한 정보나 정답이 없이 나누는 방법이다. 군집화는 비슷한 정보, 패턴이 유사한 사용자, 데이터들을 묶어주는 __패턴 인지__ 혹은 __데이터 압축__ 에 자주 사용되는 방식이다. 군집화는 다음과 같이 나뉜다.

- __hard clustering__ : 한 개체가 여러 군집에 속하는 경우를 허용하지 않는 방법 (<-> soft clustering)
- __pational clustering__ : 전체 데이터의 영역을 특정 기준에 의해 동시에 구분하는 방법 ex) K-mean clustering
- __hierarchical clustering__ : 개체들을 가까운 집단부터 차근차근 묶어나가는 방법
- __self-organizing map__ : Neural Network 기반의 군집화 방법
- __spectual clustering__ : 그래프 기반의 군집화 방법

군집화는 _정답(Label)이 주어지지 않기 때문에_ 지도학습처럼 정답과 비교하여 지표로 삼는 정확도 계산이 어려워 최적의 군집을 정확하게 파악하는 것은 어렵지만, 군집의 결과에 대한 타당성, 유용성을 따지는 __Clustering Validity Index__ (군집 타당성 지표)가 존재하여 평가를 할 수 있다. 군집화 중에서 많이 쓰이는 방법은 __Hierarchical Clustering__ (계층적 군집화)와 __K-mean Clustering__ (K-평균 군집화)이고 이에 대한 학습을 진행한다.

### _Hierarchical Clustering_ (계층적 군집화) <a name="c2"/>
__Hierarchical Clustering__ (계층적 군집화)란 _계층적 트리 모형_ 을 이용해 개별 개체들을 순차적, 계층적으로 유사한 개체 내지 그룹과 통합하여 군집화를 수행하는 알고리즘이다. _군집 간의 거리를 기반_ 으로 군집화를 수행하고, 개체들이 결합되는 순서를 나타내는 트리형태의 구조인 __Dendrogram__ (덴드로그램)을 이용하여 군집 수를 사전에 정하지 않아도 학습을 수행할 수 있다는 장점이 있다.

![Alt Text][Hierarchical Clustering]

위의 사진은 계층적 군집화를 하여 덴드로그램을 이용해 시각화를 한 것이다. 그림을 보면 처음에는 각각 다른 그룹이었다가 점차 상위 계층으로 올라 갈 수록 같은 그룹으로 그룹화가 이루어지고 최종적으로 하나의 같은 그룹으로 나오는 것을 볼 수 있다. 그림과 같이 계층적 군집화는 _단계별로 그룹화_ 를 진행하는 것을 볼 수 있다.

다음은 계층적 군집화의 장단점이다.

|장점|단점|
|---|---|
|군집의 형성되는 _과정_ 에 대해서 정확히 파악 가능|자료의 크기가 _크면_ 계산 속도가 _느려짐_ |
|군집의 수를 _명시할 필요가 없음_ |_안정성 이 낮음_|
|_덴드로그램_ 을 통한 군집화 결과 표현 : 설명, 해석 가능|_이상치 값_ 에 민감|

### __K-means Clustering__ (K-평균 군집화) <a name="c3"/>
__K-means Clustering__ (K-평균 군집화)는 _k개의 중심점_ 을 찍은 후, 중심점에서 각 점간의 _거리의 합이 최소_ 가 되는 중심점의 위치를 찾고, 이 중심점에서 가까운 점을 기준으로 묶는 군집화 알고리즘이다. 같은 중심점에 할당된 개체들은 같은 그룹을 형성하게 되며, 사전에 군집 수(k)가 정해져 있어야지 알고리즘을 수행할 수 있고 중심점은 각 군집 데이터의 _평균값_ 을 위치로 갖기 때문에 K-평균 군집화라고 이름이 붙여졌다.

![Alt Text][K-means Clustering]

위의 사진을 보면 3개의 파란색 마름모 꼴의 점(중심점)을 볼 수 있는데, 이는 3개의 군집이 존재하는 것을 나타내며, 이 중심점의 기준으로 군집이 이루어진 것을 볼 수 있다.

K-평균 군집화의 장단점은 다음과 같다.

|장점|단점|
|---|---|
|구현이 상대적으로 간단|초기에 군집 수를 결정하기 때문에 K의 값에 따라 정확도가 달라짐|
|새로운 데이터의 군집을 찾을 때 계산량이 적음|학습 초기에 군집 수를 정하기 때문에 새로운 군집 형성이 불가능|

K-평균 군집화의 군집 __중심점을 초기화__ 하는 방법은 다음과 같다. [초기화 기법 설명 자료](https://ko.wikipedia.org/wiki/K-%ED%8F%89%EA%B7%A0_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98#%EC%B4%88%EA%B8%B0%ED%99%94_%EA%B8%B0%EB%B2%95)

- __Random Partition__ (무작위 분할) : 데이터를 _무작위_ 로 군집에 할당
- __Forgy__ : _무작위로 K개를 선정_ 하여 중심점으로 설정
- __MacQueen__ : 무작위로 K개의 중심점 선정 후 _거리 순으로 군집에 할당 설정_
- __Kaufman__ : _전체 데이터의 중심점_ 을 찾고, 가장 가까운 무게중심보다 _할당되지 않은 데이터에 대해 가까이 있는 지점_ 을 중심점으로 설정

### DBSCAN <a name="c6"/>
__DBSCAN__ 은 _Density-Based Spatial Clustering of Applications with Noise_ (밀도 기반 클러스터링)의 약자이며 정확히 __'노이즈가 있는 어플리케이션을 위한 밀도에 기반한 공간적인 군집화'__ 를 뜻한다. 말 그대로 DBSCAN은 __밀도__ 에 근거한 군집화 알고리즘이며, 여기서 말하는 노이즈란 동떨어진 데이터를 말한다. 즉, _동떨어진 데이터_ 에 대한 처리는 K-평균 군집화 혹은 계층적 군집화에 비해서 높은 성능을 보인다는 것을 나타낸다. 위의 K-평균 혹은 계층적 군집화는 군집 간의 '거리' 를 이용하여 군집을 했던 반면, DBSCAN은 점이 세밀하게 몰려 있어서 어느 점을 기준으로 일정 반경 내에 점이 x개 이상 있으면 하나의 군집으로 인식하는 방식이다.

![Alt Text][dbscan]

DBSCAN은 데이터셋에 대한 탐색을 하고, '밀도'를 기반으로 다음과 같이 나누게 된다.

- __Core__ (코어) : 일정 기준 _이상_ 의 밀도를 갖는 데이터로, _코어에서 코어로 탐색_ 을 수행한다.
- __Noise__ (노이즈) : 일정 기준 _미만_ 의 밀도를 갖고, _군집에도 소속되지 않은_ 데이터이다.
- __Border__ (보더) : 일정 기준 _미만_ 의 밀도를 갖지만, _군집에 소속된_ 데이터이다. 보더를 만나면 군집에서의 탐색을 중지한다.


DBSCAN에 대한 장단점은 다음과 같다.

|장점|단점|
|---|---|
|_노이즈 처리에 강함_ |많은 연산을 수행하기 때문에 _계산 속도가 상대적으로 느림_ |
|U자형 같은 오목한 데이터, H형 모양의 같은 _기하학적 데이터 분포_ 도 쉽게 군집화 가능|_반지름과 임계치 설정_ 에 많은 영향을 받음|
|_군집의 수_ 를 정하지 않아도 됨|_차원이 높아질_ 수록 필요한 _학습 데이터의 양이 급증_ |


### Tutorial : K-평균 군집화 <a name="c4"/>
> 이미 정답(라벨)을 알고 있는 붓꽃 데이터를 바탕으로 군집화를 진행하고 그에 따른 3개의 군집이 나오는지 확인을 한다. 또한 군집화에 따른 정확도를 실제 값과 비교를 진행한다.

#### task
+ Scikit Learn 라이브러리 다운로드

#### index
+ [소스코드](#i1)
+ [결과](#i2)

##### 소스코드 <a name="i1"/>
1.필요한 라이브러리를 로드한다. 그리고나서 그래프 세션을 시작하고 붓꽃 데이터셋을 불러온다.
```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from scipy.spatial import cKDTree
from sklearn.decomposition import PCA
# PCA : Principal Component Analysis(PCA)의 약자로 한국말로 주성분 분석이라고 한다. 고차원의 데이터를 저차원으로 환원시키는 기법을 뜻한다.

sess = tf.Session()

iris = datasets.load_iris()

num_pts = len(iris.data)
num_feats = len(iris.data[0])
```

2.__그룹 개수__ 와 세대 수를 설정하고, 그래프에 필요한 라벨 변수를 생성하고, 또한 K-평균 군집화에는 중심점을 나타내는 변수가 필요하기 때문에, 임의의 세 지점을 선택하고 K-평균 알고리즘의 __초기 중심점__ 으로 사용한다.
```python
# 군집 수 및 세대 수 결정
k = 3
generations = 25

# 군집화 라벨 설정
data_points = tf.Variable(iris.data)
cluster_labels = tf.Variable(tf.zeros([num_pts], dtype=tf.int64))

# 중심점을 나타내는 변수
rand_starts = np.array([iris.data[np.random.choice(len(iris.data))] for _ in range(k)])
centroids = tf.Variable(rand_starts)
```

3.__각 데이터 지점과 중심점 사이의 거리__ 를 계산한다. 중심점과 데이터 지점들을 행렬로 확장하여 계산을 처리한다. 두 행렬 사이의 거리는 __유클리드 거리__ 를 계산한다. 그런 다음에 각  데이터 지점에서 가장 가까운, 즉, _거리 값이 가장 작은 데이터_ 가 속한 중심점으로 할당한다.
```python
# 각 데이터 지점과 중심점 사이의 거리 계산
centroids_matrix = tf.reshape(tf.tile(centroids, [num_pts, 1]), [num_pts, k, num_feats])
point_matrix = tf.reshape(tf.tile(data_points, [1, k]), [num_pts, k, num_feats])
# 유클리드 비용 함수
distances = tf.reduce_sum(tf.square(point_matrix - centroids_matrix), axis=2)

# 각 데이터 지점에서 가장 가까운 데이터가 속한 중심점으로 할당
centroids_group = tf.argmin(distances, 1)
```

4.새로운 중심점을 구하기 위한 _그룹별 평균_ 을 계산한다.
```python
# 그룹별 평균 계산 함수
def data_group_avg(group_ids, data) :
    # 그룹별 합산
    sum_total = tf.unsorted_segment_sum(data, group_ids, 3)
    # 그룹에 속한 데이터 개수
    num_total = tf.unsorted_segment_sum(tf.ones_like(data), group_ids, 3)
    # 평균 계산
    avg_by_group = sum_total/num_total
    return(avg_by_group)


means = data_group_avg(centroids_group, data_points)
update = tf.group(centroids.assign(means), cluster_labels.assign(centroids_group))
```

5.모델 변수의 초기화를 진행하고 세대 수를 반복하면서 그룹의 중심점을 갱신한다.
```python
# 모델 변수 초기화 진행
init = tf.global_variables_initializer()
sess.run(init)

# 세대를 반복하면서 중심점 갱신
for i in range(generations):
    print('Calculating gen{}, out of {}.'.format(i, generations))
    _, centroid_group_count = sess.run([update, centroids_group])
    group_count = []
    for ix in range(k):
        group_count.append(np.sum(centroid_group_count==ix))
        print('Group counts : {}'.format(group_count))

# 군집화 결과 확인
[centers, assignments] = sess.run([centroids, cluster_labels])
```

6.군집화 작업이 끝났기 때문에 결과를 확인하기 위해서 계산한 군집을 이용한 예측을 한다. 예측은 동일한 붓꽃 품종의 데이터 지점이 얼마나 많은 그룹에 속해 있는지 확인하는 작업으로 한다.

```python
def most_common(my_list) :
    return(max(set(my_list), key=my_list.count))


label0 = most_common(list(assignments[0:50]))
label1 = most_common(list(assignments[50:100]))
label2 = most_common(list(assignments[100:150]))

group0_count = np.sum(assignments[0:50]==label0)
group1_count = np.sum(assignments[50:100]==label1)
group2_count = np.sum(assignments[100:150]==label2)

accuracy = (group0_count + group1_count + group2_count)/150.

print('Accuracy: {:.2}'.format(accuracy))
```

7.시각화를 위해 PCA를 이용하여 4차원 데이터를 2차원 데이터로 변환한 다음 그래프로 표시하는 작업을 수행한다.
```python
# PCA 를 통한 4차원 -> 2차원 데이터 변환 및 그래프 표시
pca_model = PCA(n_components=2)
reduced_data = pca_model.fit_transform(iris.data)

reduced_centers = pca_model.transform(centers)

h = .02

x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

xx_pt = list(xx.ravel())
yy_pt = list(yy.ravel())
xy_pts = np.array([[x,y] for x,y in zip(xx_pt, yy_pt)])
mytree = cKDTree(reduced_centers)
dist, indexes = mytree.query(xy_pts)

indexes = indexes.reshape(xx.shape)
```

8.다음은 실제 붓꽃 데이터들의 실제 속한 그룹과 중심점을 나타낸 그래프이다.

[sklearn 라이브러리 웹사이트 데모 사이트](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html)

```python
plt.figure(1)
plt.clf()
plt.imshow(indexes, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), cmap=plt.cm.Paired, aspect='auto', origin='lower')

symbols = ['o', '^', 'D']
label_name = ['Setosa', 'Versicolour', 'Virginica']
for i in range(3):
    temp_group = reduced_data[(i*50):(50)*(i+1)]
    plt.plot(temp_group[:, 0], temp_group[:, 1], symbols[i], markersize=10, label=label_name[i])

plt.scatter(reduced_centers[:, 0], reduced_centers[:, 1], marker='x', s=169, linewidths=3, color='w', zorder=10)
plt.title('K-means clustering on Iris Dataset\n' 'Centroids are marked with white cross')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.legend(loc='lower right')
plt.show()
```

##### 결과 <a name="i2"/>

![Alt Text][k_means_graph]

그래프로 결과를 확인해보면, 결국 중심점은 __군집의 평균__ 을 나타내는 것을 볼 수 있고, 또한, 100% 정확성을 나타내지는 않지만(약 89%의 정확성을 나타냈음.) 어느 정도 군집 형성이 잘 이루어지는 것을 확인해볼 수 있다. 다음은 세대 별로 반복을 진행하면서 나타난 군집 과정이다. 각 세대 별로 군집 형성이 어떻게 이루어지 과정을 확인해 볼 수 있는 결과이다.

![Alt Text][k_means_result]

### 미해결 부분 <a name="c5"/>

#### Reference
- _Tensorflow Machine Learning Cookbook_, Nick McClure
- [Tensorflow Machine Learning Cookbook Github 사이트](https://github.com/nfmcclure/tensorflow_cookbook)
- [ratsgo's blog](https://ratsgo.github.io/machine%20learning/2017/04/16/clustering/)
- [조대협의 블로그](http://bcho.tistory.com/1203)

[Hierarchical Clustering]:https://imgur.com/mjmvOv7.png
[K-means Clustering]:https://imgur.com/xjjZi00.png
[k_means_graph]:https://imgur.com/wckayQZ.png
[k_means_result]:https://imgur.com/AZNUk2c.png
[dbscan]:https://imgur.com/aJIG0Nr.png
