## Clustering(군집화)
> __Unsupervised Learning__ (비지도학습) 방법 중 하나인 Clustering(군집화)에 대해서 학습하는 페이지이다.

#### index

1. [_Clustering_ 이란?](#c1)
2. [_Hierarchical Clustering_ (계층적 군집화)](#c2)
3. [_K-means Clustering_ (K-평균 군집화)](#c3)
4. [Tutorial : K-평균 군집화](#c4)
5. [미해결 부분](#c5)

### _Clustering_ 이란? <a name="c1"/>

__Clustering__ (군집화)란 _비지도학습_ 방법 중 하나로 비슷한 개체는 한 묶음으로, 비슷하지 않은 개체는 다른 그룹으로 __그룹화__ 하는 방법으로 그룹에 대한 정보나 정답이 없이 나누는 방법이다. 군집화는 비슷한 정보, 패턴이 유사한 사용자, 데이터들을 묶어주는 __패턴 인지__ 혹은 __데이터 압축__ 에 자주 사용되는 방식이다. 군집화는 다음과 같이 나뉜다.

- __hard clustering__ : 한 개체가 여러 군집에 속하는 경우를 허용하지 않는 방법 (<-> soft clustering)
- __pational clustering__ : 전체 데이터의 영역을 특정 기준에 의해 동시에 구분하는 방법 ex) K-mean clustering
- __hierarchical clustering__ : 개체들을 가까운 집단부터 차근차근 묶어나가는 방법
- __self-organizing map__ : Neural Network 기반의 군집화 방법
- __spectual clustering__ : 그래프 기반의 군집화 방법

군집화는 _정답(Label)이 주어지지 않기 때문에_ 지도학습처럼 정답과 비교하여 지표로 삼는 정확도 계산이 어려워 최적의 군집을 정확하게 파악하는 것은 어렵지만, 군집의 결과에 대한 타당성, 유용성을 따지는 __Clustering Validity Index__ (군집 타당성 지표)가 존재하여 평가를 할 수 있다. 군집화 중에서 많이 쓰이는 방법은 __Hierarchical Clustering__ (계층적 군집화)와 __K-mean Clustering__ (K-평균 군집화)이고 이에 대한 학습을 진행한다.

### _hierarchical Clustering_ (계층적 군집화) <a name="c2"/>
__Hierarchical Clustering__ (계층적 군집화)란 _계층적 트리 모형_ 을 이용해 개별 개체들을 순차적, 계층적으로 유사한 개체 내지 그룹과 통합하여 군집화를 수행하는 알고리즘이다. _군집 간의 거리를 기반_ 으로 군집화를 수행하고, 개체들이 결합되는 순서를 나타내는 트리형태의 구조인 __Dendrogram__ (덴드로그램)을 이용하여 군집 수를 사전에 정하지 않아도 학습을 수행할 수 있다는 장점이 있다.

![Alt Text][Hierarchical Clustering]

위의 사진은 계층적 군집화를 하여 덴드로그램을 이용해 시각화를 한 것이다. 그림을 보면 처음에는 각각 다른 그룹이었다가 점차 상위 계층으로 올라 갈 수록 같은 그룹으로 그룹화가 이루어지고 최종적으로 하나의 같은 그룹으로 나오는 것을 볼 수 있다. 그림과 같이 계층적 군집화는 _단계별로 그룹화_ 를 진행하는 것을 볼 수 있다.

### __K-means Clustering__ (K-평균 군집화) <a name="c3"/>
__K-means Clustering__ (K-평균 군집화)는 _k개의 중심점_ 을 찍은 후, 중심점에서 각 점간의 _거리의 합이 최소_ 가 되는 중심점의 위치를 찾고, 이 중심점에서 가까운 점을 기준으로 묶는 군집화 알고리즘이다. 같은 중심점에 할당된 개체들은 같은 그룹을 형성하게 되며, 사전에 군집 수(k)가 정해져 있어야지 알고리즘을 수행할 수 있고 중심점은 각 군집 데이터의 _평균값_ 을 위치로 갖기 때문에 K-평균 군집화라고 이름이 붙여졌다.

![Alt Text][K-means Clustering]

위의 사진을 보면 3개의 파란색 마름모 꼴의 점(중심점)을 볼 수 있는데, 이는 3개의 군집이 존재하는 것을 나타내며, 이 중심점의 기준으로 군집이 이루어진 것을 볼 수 있다.

### Tutorial : K-평균 군집화 <a name="c4"/>
> 이미 정답(라벨)을 알고 있는 붓꽃 데이터를 바탕으로 군집화를 진행하고 그에 따른 3개의 군집이 나오는지 확인을 한다. 또한 군집화에 따른 정확도를 실제 값과 비교를 진행한다.

#### task
+ Scikit Learn 라이브러리 다운로드

#### index
+ [소스코드](#i1)
+ [결과](#i2)

##### 소스코드 <a name="i1"/>
1.필요한 라이브러리를 로드한다. 그리고나서 그래프 세션을 시작하고 붓꽃 데이터셋을 불러온다.
```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from scipy.spatial import cKDTree
from sklearn.decomposition import PCA
# PCA : Principal Component Analysis(PCA)의 약자로 한국말로 주성분 분석이라고 한다. 고차원의 데이터를 저차원으로 환원시키는 기법을 뜻한다.

sess = tf.Session()

iris = datasets.load_iris()

num_pts = len(iris.data)
num_feats = len(iris.data[0])
```

2.__그룹 개수__ 와 세대 수를 설정하고, 그래프에 필요한 라벨 변수를 생성하고, 또한 K-평균 군집화에는 중심점을 나타내는 변수가 필요하기 때문에, 임의의 세 지점을 선택하고 K-평균 알고리즘의 __초기 중심점__ 으로 사용한다.
```python
# 군집 수 및 세대 수 결정
k = 3
generations = 25

# 군집화 라벨 설정
data_points = tf.Variable(iris.data)
cluster_labels = tf.Variable(tf.zeros([num_pts], dtype=tf.int64))

# 중심점을 나타내는 변수
rand_starts = np.array([iris.data[np.random.choice(len(iris.data))] for _ in range(k)])
centroids = tf.Variable(rand_starts)
```

3.__각 데이터 지점과 중심점 사이의 거리__ 를 계산한다. 중심점과 데이터 지점들을 행렬로 확장하여 계산을 처리한다. 두 행렬 사이의 거리는 __유클리드 거리__ 를 계산한다. 그런 다음에 각  데이터 지점에서 가장 가까운, 즉, _거리 값이 가장 작은 데이터_ 가 속한 중심점으로 할당한다.
```python
# 각 데이터 지점과 중심점 사이의 거리 계산
centroids_matrix = tf.reshape(tf.tile(centroids, [num_pts, 1]), [num_pts, k, num_feats])
point_matrix = tf.reshape(tf.tile(data_points, [1, k]), [num_pts, k, num_feats])
# 유클리드 비용 함수
distances = tf.reduce_sum(tf.square(point_matrix - centroids_matrix), axis=2)

# 각 데이터 지점에서 가장 가까운 데이터가 속한 중심점으로 할당
centroids_group = tf.argmin(distances, 1)
```

4.새로운 중심점을 구하기 위한 _그룹별 평균_ 을 계산한다.
```python
# 그룹별 평균 계산 함수
def data_group_avg(group_ids, data) :
    # 그룹별 합산
    sum_total = tf.unsorted_segment_sum(data, group_ids, 3)
    # 그룹에 속한 데이터 개수
    num_total = tf.unsorted_segment_sum(tf.ones_like(data), group_ids, 3)
    # 평균 계산
    avg_by_group = sum_total/num_total
    return(avg_by_group)


means = data_group_avg(centroids_group, data_points)
update = tf.group(centroids.assign(means), cluster_labels.assign(centroids_group))
```

5.모델 변수의 초기화를 진행하고 세대 수를 반복하면서 그룹의 중심점을 갱신한다.
```python
# 모델 변수 초기화 진행
init = tf.global_variables_initializer()
sess.run(init)

# 세대를 반복하면서 중심점 갱신
for i in range(generations):
    print('Calculating gen{}, out of {}.'.format(i, generations))
    _, centroid_group_count = sess.run([update, centroids_group])
    group_count = []
    for ix in range(k):
        group_count.append(np.sum(centroid_group_count==ix))
        print('Group counts : {}'.format(group_count))

# 군집화 결과 확인
[centers, assignments] = sess.run([centroids, cluster_labels])
```

6. 군집화 작업이 끝났기 때문에 결과를 확인하기 위해서 계산한 군집을 이용한 예측을 한다. 예측은 동일한 붓꽃 품종의 데이터 지점이 얼마나 많은 그룹에 속해 있는지 확인하는 작업으로 한다.
```python
def most_common(my_list) :
    return(max(set(my_list), key=my_list.count))


label0 = most_common(list(assignments[0:50]))
label1 = most_common(list(assignments[50:100]))
label2 = most_common(list(assignments[100:150]))

group0_count = np.sum(assignments[0:50]==label0)
group1_count = np.sum(assignments[50:100]==label1)
group2_count = np.sum(assignments[100:150]==label2)

accuracy = (group0_count + group1_count + group2_count)/150.

print('Accuracy: {:.2}'.format(accuracy))
```

7. 시각화를 위해 PCA를 이용하여 4차원 데이터를 2차원 데이터로 변환한 다음 그래프로 표시하는 작업을 수행한다.
```python
# PCA 를 통한 4차원 -> 2차원 데이터 변환 및 그래프 표시
pca_model = PCA(n_components=2)
reduced_data = pca_model.fit_transform(iris.data)

reduced_centers = pca_model.transform(centers)

h = .02

x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

xx_pt = list(xx.ravel())
yy_pt = list(yy.ravel())
xy_pts = np.array([[x,y] for x,y in zip(xx_pt, yy_pt)])
mytree = cKDTree(reduced_centers)
dist, indexes = mytree.query(xy_pts)

indexes = indexes.reshape(xx.shape)
```

8.다음은 실제 붓꽃 데이터들의 실제 속한 그룹과 중심점을 나타낸 그래프이다.

[sklearn 라이브러리 웹사이트 데모 사이트](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html)

```python
plt.figure(1)
plt.clf()
plt.imshow(indexes, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), cmap=plt.cm.Paired, aspect='auto', origin='lower')

symbols = ['o', '^', 'D']
label_name = ['Setosa', 'Versicolour', 'Virginica']
for i in range(3):
    temp_group = reduced_data[(i*50):(50)*(i+1)]
    plt.plot(temp_group[:, 0], temp_group[:, 1], symbols[i], markersize=10, label=label_name[i])

plt.scatter(reduced_centers[:, 0], reduced_centers[:, 1], marker='x', s=169, linewidths=3, color='w', zorder=10)
plt.title('K-means clustering on Iris Dataset\n' 'Centroids are marked with white cross')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.legend(loc='lower right')
plt.show()
```

##### 결과 <a name="i2"/>

### 미해결 부분 <a name="c5"/>

#### Reference
- _Tensorflow Machine Learning Cookbook_, Nick McClure
- [Tensorflow Machine Learning Cookbook Github 사이트](https://github.com/nfmcclure/tensorflow_cookbook)
- [ratsgo's blog](https://ratsgo.github.io/machine%20learning/2017/04/16/clustering/)
- [조대협의 블로그](http://bcho.tistory.com/1203)

[Hierarchical Clustering]:https://imgur.com/mjmvOv7.png
[K-means Clustering]:https://imgur.com/xjjZi00.png
